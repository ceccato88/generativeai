{
    "status": "ok",
    "data": {
        "document": {
            "id": "89d0fe9a-a30b-4f6b-860f-9b31ae54fbe3",
            "name": "2505.20368.pdf",
            "package_type": "elite",
            "ocr_type": "disable",
            "page_count": 19,
            "create_time": 1749413256
        },
        "elements": [
            {
                "type": "paragraph",
                "text": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents",
                "page": 0,
                "parent_chapter": -1,
                "index": 0,
                "outline": [
                    70.0,
                    78.0,
                    524.5,
                    108.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Jaeyoung Choe, Jihoon Kim, Woohwan Jung Department of Applied Artificial Intelligence, Hanyang University{cjy9100, skygl, whjung}@hanyang.ac.kr",
                "page": 0,
                "parent_chapter": -1,
                "index": 1,
                "outline": [
                    137.0,
                    134.0,
                    457.0,
                    175.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Abstract",
                "page": 0,
                "parent_chapter": -1,
                "index": 2,
                "outline": [
                    157.0,
                    221.5,
                    203.0,
                    232.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Retrieval-augmented generation (RAG) based large language models (LLMs) are widely used in finance for their excellent performance on knowledge-intensive tasks. However, standardized documents (e.g., SEC filing) share similar formats such as repetitive boilerplate texts, and similar table structures. This similarity forces traditional RAG methods to misidentify near-duplicate text, leading to duplicate retrieval that undermines accuracy and completeness. To address these issues, we propose the Hierarchical Retrieval with Evidence  Curation (HiREC) framework. Our approach first performs hierarchical retrieval to reduce confusion among similar texts. It first retrieve related documents and then selects the most relevant passages from the documents. The evidence curation process removes irrelevant passages. When necessary, it automatically generates complementary queries to collect missing information. To evaluate our approach, we construct and release a Large-scale Open-domain Financial (LOFin) question answering benchmark that includes 145,897 SEC documents and 1,595 question-answer pairs. Our code and data are available at https://github.com/deep-over/LOFin-bench-HiREC.",
                "page": 0,
                "parent_chapter": 2,
                "index": 3,
                "outline": [
                    84.5,
                    243.0,
                    275.5,
                    567.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "1 Introduction",
                "page": 0,
                "parent_chapter": -1,
                "index": 4,
                "outline": [
                    70.0,
                    579.5,
                    153.5,
                    589.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020) with large language models (LLMs) have significantly improved performance in knowledge-intensive tasks. Due to its ability to improve both factual accuracy and timeliness, extensive research (Yepes et al., 2024; Sarmah et al., 2024) has investigated applying RAG in searching financial information where accurate and up-todate information is crucial for decision-making.",
                "page": 0,
                "parent_chapter": 4,
                "index": 5,
                "outline": [
                    68.0,
                    599.5,
                    292.0,
                    722.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Financial documents, such as annual reports (10-K) in SEC filings, are highly structured and follow standardized templates across companies and periods, often containing similar tables and repetitive narratives. As shown in Figure 1, 2023 10-K reports from Amazon, Meta, and Walmart exhibit nearly identical table structures with similar titles and indicators, differing mainly in numerical values. Consequently, when asked, What is the difference in operating income ratio between Amazon and Walmart in 2023?, a conventional RAG system may struggle to distinguish among these similar passages, retrieving irrelevant or redundant information that leads to inaccurate answers.",
                "page": 0,
                "parent_chapter": 4,
                "index": 6,
                "outline": [
                    70.0,
                    722.5,
                    291.0,
                    775.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        6,
                        9
                    ]
                }
            },
            {
                "type": "figure",
                "text": "",
                "page": 0,
                "parent_chapter": 4,
                "index": 7,
                "outline": [
                    303.0,
                    216.0,
                    527.0,
                    422.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0
            },
            {
                "type": "paragraph",
                "text": "Figure 1: Comparison of a naive RAG approach and HiREC .",
                "page": 0,
                "parent_chapter": 4,
                "index": 8,
                "outline": [
                    305.0,
                    431.5,
                    525.0,
                    454.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "To tackle these challenges from the standardized format of financial documents, we propose the HiREC (Hierarchical Retrieval and Evidence Curation) framework. HiREC consists of two main components: hierarchical retrieval and evidence curation. The hierarchical retrieval first retrieves related documents and then selects the most relevant passages from the documents, thereby reducing confusion from near-duplicate text. As illustrated in Figure 1, narrowing the candidate set to documents from Amazon and Walmart enables the system to focus on highly relevant passages. However, the prevalence of comparative questions in financial QA often leads to incomplete retrieval of essential evidence. To address this, the evidence curation stage filters out irrelevant passages and generates complementary queries when necessary. For example if only Amazon operating income is retrieved then a complementary query is generated to fetch Walmart operating income so that all necessary evidence is gathered for an accurate answer.",
                "page": 0,
                "parent_chapter": 4,
                "index": 10,
                "outline": [
                    303.0,
                    599.5,
                    527.0,
                    777.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        10,
                        11
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "To evaluate our approach, we assess QA performance in an open-domain setting. Existing financial question-answering benchmarks (Islam et al., 2023; Lai et al., 2024) rely on small-scale corpora that include at most about 1,300 documents and very limited test sets, which do not reflect realistic financial scenarios. We propose LOFin (Largescale Open-domain Financial QA), a comprehensive financial question-answering benchmark built on a large-scale corpus containing approximately 145,000 SEC filings from companies in the S&P 500. LOFin includes 1,595 open-domain questionanswering test instances and addresses challenges in standardized document retrieval, such as nearduplicate tables and repetitive narratives, that are not evident in smaller datasets. In addition, we release the entire benchmark as open-source to support future research in the field.",
                "page": 1,
                "parent_chapter": 4,
                "index": 12,
                "outline": [
                    67.0,
                    179.5,
                    293.0,
                    425.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Experimental results indicate that HiREC improves performance by at least 13% compared to existing RAG methods. In addition, our framework consistently outperforms commercial llms with web search engines such as SearchGPT (OpenAI, 2025) and Perplexity (Perplexity, 2023).",
                "page": 1,
                "parent_chapter": 4,
                "index": 13,
                "outline": [
                    69.0,
                    425.0,
                    291.0,
                    505.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Our contributions are threefold:",
                "page": 1,
                "parent_chapter": 4,
                "index": 14,
                "outline": [
                    80.5,
                    506.5,
                    219.5,
                    517.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• We propose a hierarchical retrieval approach that retrieves related documents and identifies the most pertinent passages, thereby reducing confusion caused by near-duplicate content in standardized financial documents.",
                "page": 1,
                "parent_chapter": 4,
                "index": 15,
                "outline": [
                    82.5,
                    525.5,
                    290.0,
                    591.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• We introduce an evidence curation process that filters out irrelevant passages and generates complementary queries when necessary, effectively supplementing missing information for accurate financial QA.",
                "page": 1,
                "parent_chapter": 4,
                "index": 16,
                "outline": [
                    82.5,
                    594.5,
                    291.0,
                    660.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• We present LOFin, a large-scale, realistic financial QA benchmark that exposes challenges in standardized document retrieval, and we release it as open-source.",
                "page": 1,
                "parent_chapter": 4,
                "index": 17,
                "outline": [
                    81.5,
                    661.5,
                    291.0,
                    715.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "2 Related Works",
                "page": 1,
                "parent_chapter": -1,
                "index": 18,
                "outline": [
                    70.0,
                    727.0,
                    164.0,
                    737.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Financial QA and RAG Financial QA has advanced significantly, with recent benchmarks emphasizing numeric reasoning and table understanding. TAT-QA (Zhu et al., 2021) and FinQA (Chen et al., 2021) provide single-page tabular contexts, while DocFinQA (Reddy et al., 2024) and DocMath-Eval (Zhao et al., 2024) extend this to multi-page settings. However, these benchmarks remain closed-domain, limiting their applicability for RAG systems. Open-domain benchmarks have also been proposed. For instance, consider Financebench (Islam et al., 2023) and SEC-QA (Lai et al., 2024). However, these datasets are built on small-scale document collections and suffer from limited test set sizes or the lack of publicly available fixed test sets.",
                "page": 1,
                "parent_chapter": 18,
                "index": 19,
                "outline": [
                    69.0,
                    749.5,
                    290.0,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        19,
                        20
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "With the emergence of financial QA, research on financial RAG has also been progressing. There are graph-based methods (Barry et al., 2025; Sarmah et al., 2024) tailored to the financial domain as well as hybrid approaches (Wang, 2024). In addition, studies on financial document chunking offer valuable insights (Yepes et al., 2024).",
                "page": 1,
                "parent_chapter": 18,
                "index": 21,
                "outline": [
                    304.0,
                    263.0,
                    527.0,
                    357.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Retrieval augmented generation RAG has evolved in diverse ways (Gao et al., 2023; Zhang et al., 2025). Hierarchical retrieval was proposed for cases where sections are clearly demarcated, typically employing a two-step document-passage process (Arivazhagan et al., 2023; Chen et al., 2024). The difference from previous studies is that while they segment documents into sections, we segment at the level of individual documents. There are also studies that utilize filtering to enhance the quality of the contexts input into RAG (Zhuang et al., 2024; Wang et al., 2024b). Unlike previous approaches that required training, we manage quality using only an LLM. Iterative retrieval is typically proposed for multi-hop QA. A standard iterative method uses the context retrieved in the first step as part of the query for subsequent iterations (Trivedi et al., 2022; Shao et al., 2023). Self-RAG (Asai et al., 2023) also conducts iterative retrieval that includes the generation process. However, our approach does not utilize previously retrieved context because we specifically need to discover missing information.",
                "page": 1,
                "parent_chapter": 18,
                "index": 22,
                "outline": [
                    302.0,
                    369.0,
                    529.0,
                    682.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "3 Large-scale Open-domain Financial QA",
                "page": 1,
                "parent_chapter": -1,
                "index": 23,
                "outline": [
                    305.5,
                    698.0,
                    524.5,
                    708.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "In this section, we present LOFin, a benchmark that overcomes the limitations of current financial QA datasets by expanding the retrieval corpus and increasing open-domain QA pairs (see Table 1).",
                "page": 1,
                "parent_chapter": 23,
                "index": 24,
                "outline": [
                    305.0,
                    723.0,
                    524.5,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "3.1 Large-scale Document Collection",
                "page": 2,
                "parent_chapter": 23,
                "index": 25,
                "outline": [
                    70.0,
                    72.5,
                    249.5,
                    83.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "To reflect a real-world scenario where retrieval and QA must be performed over a large volume of documents, we collected a comprehensive set of SEC filings. Specifically, we gathered 10-K, 10-Q, and 8-K filings from the SEC EDGAR1system, covering S&P 500 companies from October 2001 to April 2025.",
                "page": 2,
                "parent_chapter": 25,
                "index": 26,
                "outline": [
                    69.0,
                    90.5,
                    291.0,
                    184.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We converted the HTML documents to PDF using the wkhtmltopdf2library. Following the approach of Islam et al., 2023, we used the PyMuPDF library3to extract text at the page level from these PDFs, excluding reports that lacked proper page separation. The final corpus consists of 145,897 reports from 516 companies.",
                "page": 2,
                "parent_chapter": 25,
                "index": 27,
                "outline": [
                    69.0,
                    185.0,
                    292.0,
                    278.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "3.2 Open-domain QA Pair Construction",
                "page": 2,
                "parent_chapter": 23,
                "index": 28,
                "outline": [
                    70.0,
                    290.0,
                    266.0,
                    301.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "In this section, we detail our process for constructing open-domain QA pairs by leveraging three established financial QA benchmarks: FinQA (Chen et al., 2021), Financebench (Islam et al., 2023), and SEC-QA (Lai et al., 2024).",
                "page": 2,
                "parent_chapter": 28,
                "index": 29,
                "outline": [
                    69.0,
                    307.0,
                    291.0,
                    373.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We begin by converting the closed-setting questions from FinQA into an open-domain format. First, we exclude any test questions for which evidence documents were not collected due to page separation or collection period issues (35 out of 1147). Next, we transform the remaining questions using GPT-4o by appending relevant period and company information. For example, the question what are the total operating expenses for 2016? is transformed into Could you provide the total operating expenses reported by Lockheed Martin for the year 2016?, with Lockheed Martin explicitly added to enhance context. The conversion prompt used is provided in Appendix B.2.",
                "page": 2,
                "parent_chapter": 28,
                "index": 30,
                "outline": [
                    67.0,
                    374.5,
                    293.0,
                    564.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Subsequently, we identify candidate evidence pages using a two-step process. First, we compute BM25 similarity scores between the FinQA gold table context and the content of each page in the candidate document, explicitly considering the distinct numerical values in the table. Then, we employ an NLI model4to measure the semantic similarity between the FinQA gold context (excluding table content) and each page. If both methods select the same top candidate page we accept it as evidence and verify its correctness. Otherwise we manually annotate the correct page to ensure accurate evidence labeling (see Appendix B.3 for further details).",
                "page": 2,
                "parent_chapter": 28,
                "index": 31,
                "outline": [
                    67.0,
                    563.0,
                    293.0,
                    714.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        31,
                        35
                    ]
                }
            },
            {
                "type": "table",
                "page": 2,
                "parent_chapter": 28,
                "index": 33,
                "outline": [
                    306.5,
                    70.5,
                    522.5,
                    168.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 1: Comparison of financial QA benchmarks. “#QAs” is the test set size, and “# Docs” is the number of documents in the retrieval corpus. SEC QA does not have a fixed QA count, as it provides a question generation framework.",
                "title_index": 34,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Benchmark"
                    },
                    "0_1": {
                        "text": " Open"
                    },
                    "0_2": {
                        "text": " Multi-Doc"
                    },
                    "0_3": {
                        "text": " # QAs"
                    },
                    "0_4": {
                        "text": " # Docs"
                    },
                    "1_0": {
                        "text": "TAT-QA"
                    },
                    "1_1": {
                        "text": ""
                    },
                    "1_2": {
                        "text": ""
                    },
                    "1_3": {
                        "text": "1,669"
                    },
                    "1_4": {
                        "text": "-"
                    },
                    "2_0": {
                        "text": "FinQA"
                    },
                    "2_1": {
                        "text": ""
                    },
                    "2_2": {
                        "text": ""
                    },
                    "2_3": {
                        "text": "1,147"
                    },
                    "2_4": {
                        "text": "-"
                    },
                    "3_0": {
                        "text": "DocFinQA"
                    },
                    "3_1": {
                        "text": ""
                    },
                    "3_2": {
                        "text": ""
                    },
                    "3_3": {
                        "text": "922"
                    },
                    "3_4": {
                        "text": "-"
                    },
                    "4_0": {
                        "text": "Financebench"
                    },
                    "4_1": {
                        "text": "✓ "
                    },
                    "4_2": {
                        "text": ""
                    },
                    "4_3": {
                        "text": "150"
                    },
                    "4_4": {
                        "text": "368"
                    },
                    "5_0": {
                        "text": "SEC-QA"
                    },
                    "5_1": {
                        "text": "✓"
                    },
                    "5_2": {
                        "text": " ✓"
                    },
                    "5_3": {
                        "text": "N/A"
                    },
                    "5_4": {
                        "text": "1,315"
                    },
                    "6_0": {
                        "text": "LOFin"
                    },
                    "6_1": {
                        "text": "✓"
                    },
                    "6_2": {
                        "text": " ✓"
                    },
                    "6_3": {
                        "text": "1,595"
                    },
                    "6_4": {
                        "text": "145,897"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        17.0,
                        32.0,
                        44.5,
                        57.0,
                        69.0,
                        83.0
                    ],
                    "columns": [
                        62.5,
                        93.0,
                        143.0,
                        176.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 1: Comparison of financial QA benchmarks. “#QAs” is the test set size, and “# Docs” is the number of documents in the retrieval corpus. SEC QA does not have a fixed QA count, as it provides a question generation framework.",
                "page": 2,
                "parent_chapter": 28,
                "index": 34,
                "outline": [
                    304.0,
                    178.5,
                    525.0,
                    238.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Financebench is designed for an open-domain setting and its questions are adopted without modification. In contrast both FinQA and Financebench mainly include single-document questions which limits their ability to evaluate multi-document retrieval and reasoning. To address this limitation, we adopt multi-document question templates from SEC-QA. We select the four that are designed for multi-document scenarios. We then manually craft the associated questions and annotate the answers along with the corresponding evidence pages. This process enhances our open-domain QA pairs with challenges that require multi-document and multihop reasoning.",
                "page": 2,
                "parent_chapter": 28,
                "index": 36,
                "outline": [
                    303.0,
                    283.5,
                    528.0,
                    474.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Following the ACL ARR review, we finalized the LOFin benchmark with a total of 1,595 QA pairs (initially 1,389), reflecting the addition of 205 newly created QA pairs based on recent SEC filings. We refer to the initial version as LOFin-1.4k and the expanded version as LOFin-1.6k to clearly distinguish between the two. These newly added questions follow the same annotation protocol and are designed to promote more complex reasoning across multiple documents. For details of the expanded LOFin-1.6k benchmark, see Appendix A; for the SEC-QA templates used in its construction, refer to Appendix B.4.",
                "page": 2,
                "parent_chapter": 28,
                "index": 37,
                "outline": [
                    303.0,
                    474.0,
                    528.0,
                    650.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4 Hierarchical Retrieval with Evidence Curation (HiREC) Framework",
                "page": 2,
                "parent_chapter": -1,
                "index": 38,
                "outline": [
                    305.5,
                    660.5,
                    512.5,
                    686.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "In this section, we introduce the HiREC framework. Figure 2 shows the overall framework and process. The framework comprises two main components: hierarchical retrieval and evidence curation. During the hierarchical retrieval stage a hierarchical approach retrieves passages \\(\\mathcal{P}_{r}\\) that are relevant to the question q. In the evidence curation process the retrieved passages are filtered to retain only those directly pertinent and then evaluated to determine whether they provide sufficient information to answer the question. If the information is insufficient a complementary question \\(q_{c}\\) is generated to reinitiate the retrieval process (complementary pass). Otherwise if the evidence is sufficient the filtered passage set \\(\\mathcal{P}_{f}\\) is forwarded to the Answer Generator to produce the final answer (main pass). When the maximum iteration \\(i_{\\mathrm{max}}\\) is reached evidence curation halts and the passages retrieved using qc are merged with the previously filtered passages to generate the answer. The pseudocode of HiREC is described in Algorithm 1. For the LLM-based components of our framework, we use instructionstyle prompts tailored to each module’s objective. Appendix D provides the full prompt templates and design process.",
                "page": 2,
                "parent_chapter": 38,
                "index": 39,
                "outline": [
                    304.0,
                    695.0,
                    527.0,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        39,
                        42
                    ]
                }
            },
            {
                "type": "figure",
                "text": "",
                "page": 3,
                "parent_chapter": 38,
                "index": 40,
                "outline": [
                    73.5,
                    69.0,
                    524.5,
                    170.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0
            },
            {
                "type": "paragraph",
                "text": "Figure 2: Overview of hierarchical retrieval with evidence curation framework",
                "page": 3,
                "parent_chapter": 38,
                "index": 41,
                "outline": [
                    140.5,
                    183.0,
                    454.5,
                    194.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.1 Hierarchical Retrieval",
                "page": 3,
                "parent_chapter": 38,
                "index": 43,
                "outline": [
                    70.0,
                    471.5,
                    199.5,
                    481.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Standardized documents use uniform templates with repetitive structures and similar content, which makes retrieving distinct and relevant passages challenging. We address this issue using a hierarchical approach. First we retrieve documents relevant to the question (4.1.1) to narrow the search space, and then we select pertinent passages within those documents (4.1.2).",
                "page": 3,
                "parent_chapter": 43,
                "index": 44,
                "outline": [
                    69.0,
                    490.5,
                    291.0,
                    598.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.1.1 Document Retriever",
                "page": 3,
                "parent_chapter": 43,
                "index": 45,
                "outline": [
                    70.0,
                    610.5,
                    197.5,
                    619.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Document indexing. Documents contain extensive context, and their standardized format makes it difficult to capture all important details in a single vector. To address this, we extract and index key distinguishing information when retrieving documents. In financial reports, the cover page provides essential details such as the company name, report type, and fiscal period. For each document \\(d\\in\\mathcal{D}.\\), we generate a cover page summary \\(d^{\\prime}\\) using an LLM (the prompt is detailed in Appendix D.1), precompute its embedding with a bi-encoder (Wang et al., 2024a), and index the resulting vector as \\(\\mathbf{v}_{d}=E^{D}(d^{\\prime})\\) in the document store.",
                "page": 3,
                "parent_chapter": 45,
                "index": 46,
                "outline": [
                    67.0,
                    626.5,
                    293.0,
                    777.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        46,
                        48
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Algorithm 1 HiREC framework\nRequire: A question q, a corpus \\(\\mathcal{D}\\), maximum number of\niterations \\(_{i_{m a x}}\\) \n1: \\(\\mathcal{P}_{f}\\gets\\emptyset\\) // Initialization\n2: \\(\\mathcal{P}_{r}\\gets\\mathrm{HIERARC}\\) HICALRETRIE \\(\\mathrm{{3VAl}}\\) L \\((q,\\mathcal{D})\\) // Sec 4.1\n3: for \\(i=1...i_{m a x}\\) do\n4: \\((\\mathcal{P}_{f},q_{c},y)\\leftarrow\\mathrm{EvIDENCECURATION}(q,\\mathcal{P}_{r})\\,.\\)// Sec 4.2\n5: if y is Answerable then\n6: return A \\(\\mathrm{NS}\\) WERG \\(\\mathbf{\\mathrm{ENER}}\\)\\({}^{\\prime}\\!\\mathrm{A}^{\\prime}\\) TIO \\(\\mathrm{v}(q,\\mathcal{P}_{f})\\) // Sec 4.3\n7: end if\n8: \\(\\mathcal{P}_{r}\\gets\\mathrm{HIERARCHICALRI}\\) ETRIEVAL \\((q_{c},\\mathcal{D})\\) // Sec 4.1\n9: \\(\\mathcal{P}_{r}\\leftarrow\\mathcal{P}_{r}\\cup\\mathcal{P}_{f}\\) \n10: end for\n11: return \\(\\mathrm{ANSWERGENERATION}(q,\\mathcal{P}_{r})\\) // Sec 4.3",
                "page": 3,
                "parent_chapter": 45,
                "index": 47,
                "outline": [
                    302.0,
                    201.5,
                    527.0,
                    352.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Document retrieval process. Our document retrieval process consists of three stages. First, given a question q, we use an LLM to convert it into a refined query \\(q^{\\prime}.\\). This conversion reduces issues caused by extraneous financial terms (e.g., tickers or service names like Google or Facebook) that may hinder effective retrieval. (Details of the transformation prompt are provided in Appendix D.2.) Second, we perform dense retrieval. We compute the vector representation of \\(\\dot{q}^{\\prime}\\) using the same bi-encoder employed during document indexing, yielding \\({\\bf v}_{q^{\\prime}}\\,=\\,E^{D}(q^{\\prime})\\). The relevance between \\(q^{\\prime}\\) and each document d is then determined via \\(s_{q^{\\prime},d}^{D}=\\mathbf{v}_{q^{\\prime}}^{\\top}\\mathbf{v}_{d}\\) (Karpukhin et al., 2020); based on these scores, we retrieve \\(k_{D}^{\\prime}\\) candidate documents \\(\\mathcal{D}_{\\mathrm{cand}}.\\). Finally, we rerank these candidates using a cross-encoder (He et al., 2021) by computing \\(\\mathrm{CrossEncoder}^{D}(q^{\\prime},d)\\) and select the top \\(k_{D}\\) documents \\(\\mathcal{D}_{r}\\). This multi-stage process ultimately produces the final set of documents \\(\\mathcal{D}_{r}\\) that are most relevant to the original question q.",
                "page": 3,
                "parent_chapter": 45,
                "index": 49,
                "outline": [
                    301.0,
                    393.5,
                    529.0,
                    681.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.1.2 Passage Retriever",
                "page": 3,
                "parent_chapter": 43,
                "index": 50,
                "outline": [
                    305.0,
                    691.0,
                    422.5,
                    703.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Within the final set of retrieved documents \\(\\mathcal{D}_{r}\\), the passage retriever evaluates each passage p by computing a score using a cross-encoder, \\(\\mathbf{CrossEncoder}^{P}(q,p)\\). It then selects the top \\(k_{P}\\) passages to form the final set \\(\\mathcal{P}_{r}\\). By reducing the number of passages processed by the cross-encoder, we enable real-time computation while still taking advantage of its superior ability to capture intersentence relationships compared to a bi-encoder. However, passage retrievers that are pretrained on general text typically have difficulty handling financial tables. For instance, when retrieving tables, attributes such as titles, periods, and indicators are more important than the numerical values, yet these cues are not well captured. To address this, we finetuned the model on table data using FinQA training set where tables serve as evidence. Specifically, for each question q and its associated evidence document d, we denote the tables on the evidence page as the evidence passage set \\(\\chi.\\). For each evidence passage \\(p\\in\\mathcal{X}\\), we sample \\(n_{n e g}\\) negative passages, where negative passages are defined as tables that appear on pages other than the evidence page. The cross-encoder is then trained with a binary crossentropy loss (Nogueira and Cho, 2019) defined as",
                "page": 3,
                "parent_chapter": 50,
                "index": 51,
                "outline": [
                    303.0,
                    709.0,
                    526.0,
                    776.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        51,
                        52
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "\\[\\begin{array}{l}{{\\displaystyle{\\mathcal{L}}=\\sum_{(q,p)\\in{\\mathcal{X}}}\\Bigg[-\\log\\bigl({\\mathrm{CrossEncoder}}^{P}(q,p)\\bigr)}}\\\\ {{\\displaystyle~~~~~~~~~~~~-\\sum_{p^{\\prime}\\in{\\mathcal{P}}^{-}}\\log\\Bigl(1-{\\mathrm{CrossEncoder}}^{P}(q,p^{\\prime})\\Bigr)\\Bigg],}}\\end{array}\\]",
                "page": 4,
                "parent_chapter": 50,
                "index": 53,
                "outline": [
                    70.0,
                    345.5,
                    287.5,
                    410.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "where the cross-encoder applies an internal sigmoid to produce scores in [0, 1].",
                "page": 4,
                "parent_chapter": 50,
                "index": 54,
                "outline": [
                    68.0,
                    413.5,
                    289.5,
                    440.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.2 Evidence Curation",
                "page": 4,
                "parent_chapter": 38,
                "index": 55,
                "outline": [
                    70.0,
                    450.5,
                    183.0,
                    459.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Financial questions often involve comparisons between different time periods or companies. Even when the retrieval process selects relevant passages, some critical information may be missing. Furthermore, retrieved passages can contain irrelevant data that hinders overall performance (Liu et al., 2024;Xu et al., 2024). To overcome these issues, we introduce an evidence curation process that filters out incorrect data and fills information gaps by initiating additional retrieval when necessary. Our process comprises three modules: a passage filter that removes irrelevant passages (Section 4.2.1), an answerability checker that assesses evidence sufficiency (Section 4.2.2), and a complementary question generator that formulates a supplementary query if necessary (Section 4.2.3). We use an LLM to perform all three tasks in a single response (see Appendix D.3 for the prompt).",
                "page": 4,
                "parent_chapter": 55,
                "index": 56,
                "outline": [
                    66.0,
                    466.0,
                    294.0,
                    712.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.2.1 Passage Filter",
                "page": 4,
                "parent_chapter": 55,
                "index": 57,
                "outline": [
                    70.0,
                    720.0,
                    168.5,
                    732.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The passage filter removes passages from the retrieved set \\(\\mathcal{P}_{r}\\) that are not relevant to the question, yielding a filtered passage set \\(\\mathcal{P}_{f}\\) containing at most \\(k_{P}^{\\prime}\\) passages. The filter considers both newly retrieved passages and those previously identified as relevant from earlier iterations. This step is crucial, as the inclusion of noisy, irrelevant passages can lead the LLM to generate inaccurate responses.",
                "page": 4,
                "parent_chapter": 57,
                "index": 58,
                "outline": [
                    68.0,
                    736.5,
                    291.0,
                    776.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        58,
                        59
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "4.2.2 Answerability Checker",
                "page": 4,
                "parent_chapter": 55,
                "index": 60,
                "outline": [
                    305.0,
                    149.5,
                    446.5,
                    160.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The answerability checker evaluates whether the filtered passages \\(\\mathcal{P}_{f}\\) provide sufficient evidence to answer the question. If they are deemed adequate, \\(\\mathcal{P}_{f}\\) and the question are forwarded to the answer generation stage; otherwise, the lack of sufficient information triggers a complementary iteration to retrieve additional data.",
                "page": 4,
                "parent_chapter": 60,
                "index": 61,
                "outline": [
                    304.0,
                    165.0,
                    526.0,
                    256.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.2.3 Complementary Question Generator",
                "page": 4,
                "parent_chapter": 55,
                "index": 62,
                "outline": [
                    305.0,
                    268.5,
                    510.5,
                    281.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The complementary question generator examines the filtered passages \\(\\mathcal{P}_{f}\\) to identify gaps in the evidence needed to answer the question. It then generates a supplementary query \\(q_{c}\\), which is used as input for the next retrieval.",
                "page": 4,
                "parent_chapter": 62,
                "index": 63,
                "outline": [
                    305.0,
                    284.5,
                    526.0,
                    351.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "4.3 Answer Generation",
                "page": 4,
                "parent_chapter": 38,
                "index": 64,
                "outline": [
                    305.0,
                    363.5,
                    422.0,
                    372.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "In the answer generation stage, the relevant passages and the original question serve as inputs to a reasoning process that derives the final answer. For questions requiring numerical calculations, a Program-of-Thought (PoT) (Chen et al., 2022) reasoning method is employed, while for text-based inferential questions, a Chain-of-Thought (CoT) (Wei et al., 2022) approach is applied. This dual strategy is particularly effective for financial documents, which are rich in numerical data and tables, ensuring comprehensive and accurate reasoning. See Appendix D.4 for prompt details, adapted from DocMath-Eval (Zhao et al., 2024).",
                "page": 4,
                "parent_chapter": 64,
                "index": 65,
                "outline": [
                    303.0,
                    379.0,
                    528.0,
                    556.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5 Experiments",
                "page": 4,
                "parent_chapter": -1,
                "index": 66,
                "outline": [
                    305.0,
                    567.5,
                    389.0,
                    580.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.1 Experimental Settings",
                "page": 4,
                "parent_chapter": 66,
                "index": 67,
                "outline": [
                    305.0,
                    586.5,
                    433.5,
                    597.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Dataset. We evaluate the open-domain QA methods on the LOFin benchmark proposed in Section 3. All main experiments in this section are conducted on LOFin-1.4k , which contains 1,389 QA pairs. Results on the updated benchmark LOFin-1.6k are reported separately in Appendix A.",
                "page": 4,
                "parent_chapter": 67,
                "index": 68,
                "outline": [
                    304.0,
                    604.0,
                    526.0,
                    683.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "For the purpose of the main experimental analysis, we categorize the question-answer pairs into three groups based on their format and context:",
                "page": 4,
                "parent_chapter": 67,
                "index": 69,
                "outline": [
                    304.0,
                    686.5,
                    525.0,
                    724.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Numeric (Table): The answer is a number from a table or can be calculated from numbers in tables.",
                "page": 4,
                "parent_chapter": 67,
                "index": 70,
                "outline": [
                    317.5,
                    737.0,
                    526.0,
                    773.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 5,
                "parent_chapter": 67,
                "index": 71,
                "outline": [
                    73.5,
                    71.5,
                    520.5,
                    235.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 2: Main evaluation results on LOFin-1.4k . Methods marked with ♢ use GPT-4o for generation. Here, k denotes the average number of input passages used during generation, and Gold evidence shows only the correct page. Bold indicates the highest performance.",
                "title_index": 72,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Dataset"
                    },
                    "0_1": {
                        "text": " Numeric (Table)"
                    },
                    "0_3": {
                        "text": " Numeric (Text)"
                    },
                    "0_5": {
                        "text": " Textual"
                    },
                    "0_7": {
                        "text": " Average"
                    },
                    "1_0": {
                        "text": "Method"
                    },
                    "1_1": {
                        "text": "Page\nRecall"
                    },
                    "1_2": {
                        "text": "Answer\nAcc"
                    },
                    "1_3": {
                        "text": "Page\nRecall"
                    },
                    "1_4": {
                        "text": "Answer\nAcc"
                    },
                    "1_5": {
                        "text": "Page\nRecall"
                    },
                    "1_6": {
                        "text": "Answer\nAcc"
                    },
                    "1_7": {
                        "text": "Page\nRecall"
                    },
                    "1_8": {
                        "text": "Answer\nAcc"
                    },
                    "1_9": {
                        "text": "k"
                    },
                    "2_0": {
                        "text": "GPT-4o (Zero-shot)"
                    },
                    "2_1": {
                        "text": "-"
                    },
                    "2_2": {
                        "text": "3.82"
                    },
                    "2_3": {
                        "text": "-"
                    },
                    "2_4": {
                        "text": "2.93"
                    },
                    "2_5": {
                        "text": "-"
                    },
                    "2_6": {
                        "text": "35.00"
                    },
                    "2_7": {
                        "text": "-"
                    },
                    "2_8": {
                        "text": "13.92"
                    },
                    "2_9": {
                        "text": "-"
                    },
                    "3_0": {
                        "text": "Perplexity (Perplexity, 2023)"
                    },
                    "3_1": {
                        "text": "-"
                    },
                    "3_2": {
                        "text": "2.51"
                    },
                    "3_3": {
                        "text": "-"
                    },
                    "3_4": {
                        "text": "5.13"
                    },
                    "3_5": {
                        "text": "-"
                    },
                    "3_6": {
                        "text": "24.00"
                    },
                    "3_7": {
                        "text": "-"
                    },
                    "3_8": {
                        "text": "10.55"
                    },
                    "3_9": {
                        "text": "-"
                    },
                    "4_0": {
                        "text": "Self-RAG (Asai et al., 2023)"
                    },
                    "4_1": {
                        "text": "19.96"
                    },
                    "4_2": {
                        "text": "1.86"
                    },
                    "4_3": {
                        "text": "24.18"
                    },
                    "4_4": {
                        "text": "4.03"
                    },
                    "4_5": {
                        "text": "12.75"
                    },
                    "4_6": {
                        "text": "17.00"
                    },
                    "4_7": {
                        "text": "18.96"
                    },
                    "4_8": {
                        "text": "7.63"
                    },
                    "4_9": {
                        "text": "10.0"
                    },
                    "5_0": {
                        "text": "RQ-RAG (Chan et al., 2024)"
                    },
                    "5_1": {
                        "text": "18.61"
                    },
                    "5_2": {
                        "text": "1.97"
                    },
                    "5_3": {
                        "text": "19.05"
                    },
                    "5_4": {
                        "text": "2.56"
                    },
                    "5_5": {
                        "text": "17.96"
                    },
                    "5_6": {
                        "text": "20.50"
                    },
                    "5_7": {
                        "text": "18.54"
                    },
                    "5_8": {
                        "text": "8.34"
                    },
                    "5_9": {
                        "text": "36.0"
                    },
                    "6_0": {
                        "text": "IRCoT (Trivedi et al., 2022) \\(\\diamondsuit\\)"
                    },
                    "6_1": {
                        "text": "28.17"
                    },
                    "6_2": {
                        "text": "19.10"
                    },
                    "6_3": {
                        "text": "34.62"
                    },
                    "6_4": {
                        "text": "27.84"
                    },
                    "6_5": {
                        "text": "12.67"
                    },
                    "6_6": {
                        "text": "20.00"
                    },
                    "6_7": {
                        "text": "25.15"
                    },
                    "6_8": {
                        "text": "22.31"
                    },
                    "6_9": {
                        "text": "20.0"
                    },
                    "7_0": {
                        "text": "HybridSearch (Wang, 2024) \\(\\diamondsuit\\)"
                    },
                    "7_1": {
                        "text": "26.75"
                    },
                    "7_2": {
                        "text": "19.10"
                    },
                    "7_3": {
                        "text": "32.05"
                    },
                    "7_4": {
                        "text": "30.77"
                    },
                    "7_5": {
                        "text": "14.37"
                    },
                    "7_6": {
                        "text": "27.00"
                    },
                    "7_7": {
                        "text": "24.39"
                    },
                    "7_8": {
                        "text": "25.62"
                    },
                    "7_9": {
                        "text": "10.0"
                    },
                    "8_0": {
                        "text": "HHR (Arivazhagan et al., 2023) \\(\\diamondsuit\\)"
                    },
                    "8_1": {
                        "text": "37.67"
                    },
                    "8_2": {
                        "text": "26.53"
                    },
                    "8_3": {
                        "text": "40.29"
                    },
                    "8_4": {
                        "text": "32.97"
                    },
                    "8_5": {
                        "text": "21.98"
                    },
                    "8_6": {
                        "text": "26.50"
                    },
                    "8_7": {
                        "text": "33.31"
                    },
                    "8_8": {
                        "text": "28.67"
                    },
                    "8_9": {
                        "text": "10.0"
                    },
                    "9_0": {
                        "text": "Dense (Karpukhin et al., 2020) \\(\\diamondsuit\\)"
                    },
                    "9_1": {
                        "text": "37.69"
                    },
                    "9_2": {
                        "text": "23.69"
                    },
                    "9_3": {
                        "text": "40.48"
                    },
                    "9_4": {
                        "text": "32.97"
                    },
                    "9_5": {
                        "text": "26.18"
                    },
                    "9_6": {
                        "text": "31.00"
                    },
                    "9_7": {
                        "text": "34.78"
                    },
                    "9_8": {
                        "text": "29.22"
                    },
                    "9_9": {
                        "text": "10.0"
                    },
                    "10_0": {
                        "text": "HiREC (Ours) \\(\\diamond\\)"
                    },
                    "10_1": {
                        "text": "50.17"
                    },
                    "10_2": {
                        "text": "37.23"
                    },
                    "10_3": {
                        "text": "53.48"
                    },
                    "10_4": {
                        "text": "48.35"
                    },
                    "10_5": {
                        "text": "32.39"
                    },
                    "10_6": {
                        "text": "41.50"
                    },
                    "10_7": {
                        "text": "45.35"
                    },
                    "10_8": {
                        "text": "42.36"
                    },
                    "10_9": {
                        "text": "3.7"
                    },
                    "11_0": {
                        "text": "Gold evidence \\(\\diamondsuit\\)"
                    },
                    "11_1": {
                        "text": "100"
                    },
                    "11_2": {
                        "text": "64.96"
                    },
                    "11_3": {
                        "text": "100"
                    },
                    "11_4": {
                        "text": "69.23"
                    },
                    "11_5": {
                        "text": "100"
                    },
                    "11_6": {
                        "text": "63.50"
                    },
                    "11_7": {
                        "text": "100"
                    },
                    "11_8": {
                        "text": "65.90"
                    },
                    "11_9": {
                        "text": "1.3"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ]
                    ],
                    [
                        [
                            0,
                            3
                        ],
                        [
                            0,
                            4
                        ]
                    ],
                    [
                        [
                            0,
                            5
                        ],
                        [
                            0,
                            6
                        ]
                    ],
                    [
                        [
                            0,
                            7
                        ],
                        [
                            0,
                            8
                        ],
                        [
                            0,
                            9
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        15.7,
                        41.9,
                        55.9,
                        66.9,
                        77.4,
                        88.5,
                        99.5,
                        111.2,
                        122.8,
                        133.9,
                        146.7
                    ],
                    "columns": [
                        131.5,
                        165.9,
                        204.3,
                        238.6,
                        277.0,
                        310.8,
                        350.4,
                        383.6,
                        423.1
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 2: Main evaluation results on LOFin-1.4k . Methods marked with \\(\\diamondsuit\\) use GPT-4o for generation. Here, k denotes the average number of input passages used during generation, and Gold evidence shows only the correct page. Bold indicates the highest performance.",
                "page": 5,
                "parent_chapter": 67,
                "index": 72,
                "outline": [
                    69.0,
                    243.0,
                    525.0,
                    279.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Numeric (Text): The answer is a number derived by extracting and combining numerical information from text.",
                "page": 5,
                "parent_chapter": 67,
                "index": 73,
                "outline": [
                    82.5,
                    287.5,
                    291.0,
                    324.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Textual: The answer is a textual explanation.",
                "page": 5,
                "parent_chapter": 67,
                "index": 74,
                "outline": [
                    82.5,
                    329.0,
                    290.0,
                    340.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Examples and statistics for these categories are in Table 12.",
                "page": 5,
                "parent_chapter": 67,
                "index": 75,
                "outline": [
                    70.0,
                    352.5,
                    289.5,
                    376.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Implementation details. Our system leverages several pre-trained and fine-tuned models across its components. For the passage retriever, we finetune a DeBERTa-v3 model (He et al., 2021) with \\(n_{n e g}\\,=\\,8\\) for negative sampling, a batch size of 128, 3 epochs, and a learning rate of \\(2\\times10^{-7}\\) on a single GeForce RTX 4090 GPU. For document retrieval, we employ the E5 model (Wang et al., 2024a) as the bi-encoder and use DeBERTa-v3 as the reranker. The answer generator is powered by OpenAI’s GPT-4o, while other LLM-based tasks (query transformation, document summarization, and evidence curation) are handled by Qwen-2.5-7B-Instruct (Yang et al., 2024).",
                "page": 5,
                "parent_chapter": 67,
                "index": 76,
                "outline": [
                    67.0,
                    387.0,
                    294.0,
                    577.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Our framework runs for a maximum of \\(i_{\\mathrm{max}}=3\\) iterations. The document retriever retrieves \\(k_{D}=\\) 5 documents, and the passage retriever extracts \\(k_{P}=5\\) passages, with the passage filter capping the output at \\(k_{P}^{\\prime}=10\\). Additional hyperparameters are provided in Appendix C.1.",
                "page": 5,
                "parent_chapter": 67,
                "index": 77,
                "outline": [
                    69.0,
                    578.5,
                    290.0,
                    658.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Baseline methods. We compare our proposed method with several state-of-the-art RAG approaches, including RQ-RAG (Chan et al., 2024) and Self-RAG (Asai et al., 2023). To ensure a fair comparison with our approach—which employs GPT-4o as the answer generator—we use the same answer generator for the latest retrieval algorithms: Dense, HybridSearch (Wang, 2024), HHR (Arivazhagan et al., 2023), and IRCoT (Trivedi et al., 2022). In addition, we evaluate against Perplexity (Perplexity, 2023), a commercial LLM service that combines web search with LLMs. RQ-RAG, Self-RAG, and IRCoT employ iterative LLM-based retrieval, while Dense serves as a strong baseline, it employs OpenAI’s text-embedding-3-small as its encoder for DPR, with results reranked by DeBERTa-v3. All methods construct passages by concatenating the title and content during retrieval. Detailed configurations for each baseline are provided in Appendix C.2.",
                "page": 5,
                "parent_chapter": 67,
                "index": 78,
                "outline": [
                    68.0,
                    668.5,
                    291.0,
                    776.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        78,
                        79
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Metrics. We evaluate numeric answers for accuracy by considering rounding and truncation, while textual answers are evaluated using GPT-4o and FAMMA prompts (Xue et al., 2024) (see Appendix C.3). We measure retrieval performance at the page level, using recall and precision against groundtruth evidence pages as metrics. In particular, since chunk locations or units can vary for Page recall, we standardize them at the page level to ensure consistent performance measurement.",
                "page": 5,
                "parent_chapter": 67,
                "index": 80,
                "outline": [
                    304.0,
                    459.5,
                    527.0,
                    596.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.2 Main Result",
                "page": 5,
                "parent_chapter": 66,
                "index": 81,
                "outline": [
                    305.0,
                    608.5,
                    388.0,
                    618.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Table 2 shows the retrieval performance (page recall) and final answer accuracy for HiREC and the baselines. Our approach outperforms all baselines, achieving at least 10% higher page recall and 13% higher answer accuracy than the second-best model, Dense. The result validates the effectiveness of our method in retrieval for standardized documents. Furthermore, HiREC retrieves an average of only 3.7 passages, demonstrating its efficiency in selecting high-quality evidence through evidence curation.",
                "page": 5,
                "parent_chapter": 81,
                "index": 82,
                "outline": [
                    304.0,
                    626.5,
                    527.0,
                    774.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 6,
                "parent_chapter": 81,
                "index": 83,
                "outline": [
                    71.5,
                    70.0,
                    287.5,
                    152.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 3: Ablation study",
                "title_index": 84,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Method"
                    },
                    "0_1": {
                        "text": "Page\nPrecision"
                    },
                    "0_2": {
                        "text": "Page\nRecall"
                    },
                    "0_3": {
                        "text": "Ansswer\nAccuracy"
                    },
                    "1_0": {
                        "text": "HiREC"
                    },
                    "1_1": {
                        "text": "21.79"
                    },
                    "1_2": {
                        "text": "45.35"
                    },
                    "1_3": {
                        "text": "42.36"
                    },
                    "2_0": {
                        "text": "w/o HR"
                    },
                    "2_1": {
                        "text": "14.75"
                    },
                    "2_2": {
                        "text": "34.16"
                    },
                    "2_3": {
                        "text": "32.76"
                    },
                    "3_0": {
                        "text": "w/o EC"
                    },
                    "3_1": {
                        "text": "4.70"
                    },
                    "3_2": {
                        "text": "41.41"
                    },
                    "3_3": {
                        "text": "36.70"
                    },
                    "4_0": {
                        "text": "w/o Fine-tuning"
                    },
                    "4_1": {
                        "text": "21.07"
                    },
                    "4_2": {
                        "text": "42.77"
                    },
                    "4_3": {
                        "text": "40.13"
                    },
                    "5_0": {
                        "text": "w/o Filter"
                    },
                    "5_1": {
                        "text": "8.43"
                    },
                    "5_2": {
                        "text": "50.19"
                    },
                    "5_3": {
                        "text": "42.08"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        25.0,
                        34.5,
                        45.5,
                        55.5,
                        66.5
                    ],
                    "columns": [
                        78.0,
                        127.5,
                        165.0
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 3: Ablation study",
                "page": 6,
                "parent_chapter": 81,
                "index": 84,
                "outline": [
                    131.5,
                    160.5,
                    227.0,
                    171.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "figure",
                "text": "",
                "page": 6,
                "parent_chapter": 81,
                "index": 85,
                "outline": [
                    71.5,
                    187.0,
                    288.5,
                    313.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0
            },
            {
                "type": "paragraph",
                "text": "Figure 3: Comparison of company, document, page error rates for HiREC and baselines.",
                "page": 6,
                "parent_chapter": 81,
                "index": 86,
                "outline": [
                    69.0,
                    311.0,
                    288.5,
                    332.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Notably, in the textual category, the answer accuracies are higher than the page recalls for all methods. This suggests that LLMs can often answer text-based questions correctly even when retrieval is incomplete. Numeric questions require more precise reasoning and the table category remains especially challenging.",
                "page": 6,
                "parent_chapter": 81,
                "index": 87,
                "outline": [
                    69.0,
                    342.0,
                    292.0,
                    436.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.3 Analysis",
                "page": 6,
                "parent_chapter": 66,
                "index": 88,
                "outline": [
                    70.0,
                    447.0,
                    134.0,
                    459.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Ablation Study Table 3 shows page precision, page recall and answer accuracy for HiREC when each component is removed. Hierarchical retrieval (HR) and evidence curation (EC) denote the two components. The setting w/o HR corresponds to the outcome of applying the Dense method in conjunction with EC, while the setting w/o EC represents the initial retrieval performance of HR (\\(k P=10\\)).",
                "page": 6,
                "parent_chapter": 88,
                "index": 89,
                "outline": [
                    67.0,
                    463.5,
                    291.0,
                    586.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The w/o Fine-tuning setting uses an unfine-tuned reranker to address fairness concerns related to table-specific prior knowledge. Despite a slight drop in performance, HiREC still outperforms the Dense baseline by over 10% in accuracy. Even when Dense is paired with a fine-tuned reranker, it only reaches an average AnswerAcc of 30.55%, maintaining a similar performance gap.",
                "page": 6,
                "parent_chapter": 88,
                "index": 90,
                "outline": [
                    68.0,
                    586.5,
                    292.0,
                    694.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "HR is critical for enhancing retrieval accuracy because its absence produces the lowest performance. In addition, the initial search performance of HR exceeds that of the dense method combined with EC. The w/o EC setting demonstrates the effectiveness of both passage filter and the complementary question generator. HR supported by EC delivers enhanced precision, recall and answer accuracy. Furthermore, results from the w/o Filter setting reveal the advantages of the complement and filtering. Although the complementary component attains the highest recall score in the absence of filtering, its accuracy is lower than that of HiREC with filtering. This outcome is attributed to the inclusion of incorrect information that causes conflicts when filtering is not applied (Xu et al., 2024).",
                "page": 6,
                "parent_chapter": 88,
                "index": 91,
                "outline": [
                    69.0,
                    695.5,
                    291.0,
                    775.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        91,
                        96
                    ]
                }
            },
            {
                "type": "figure",
                "text": "",
                "page": 6,
                "parent_chapter": 88,
                "index": 92,
                "outline": [
                    308.5,
                    75.5,
                    519.0,
                    172.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0
            },
            {
                "type": "paragraph",
                "text": "Figure 4: Recall, precision, and passages per query by iteration. EC stands for Evidence curation.",
                "page": 6,
                "parent_chapter": 88,
                "index": 93,
                "outline": [
                    305.0,
                    175.0,
                    524.5,
                    197.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "figure",
                "text": ",",
                "page": 6,
                "parent_chapter": 88,
                "index": 94,
                "outline": [
                    327.5,
                    216.0,
                    504.5,
                    337.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0
            },
            {
                "type": "paragraph",
                "text": "Figure 5: Precision-recall curve (recall on X-axis, precision on Y-axis)",
                "page": 6,
                "parent_chapter": 88,
                "index": 95,
                "outline": [
                    305.0,
                    334.5,
                    525.0,
                    357.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Error type analysis. Figure 3 presents the error count for cases in which the company indicated in the document is incorrect for the retrieved passage of each method. A company error is defined as an instance in which both the passage and the document contain incorrect company information, and the results are reported based solely on the top-1 retrieved passage. HiREC achieves the fewest errors by correctly identifying companies during document retrieval, which in turn ensures accurate passage retrieval.",
                "page": 6,
                "parent_chapter": 88,
                "index": 97,
                "outline": [
                    303.0,
                    516.0,
                    528.0,
                    666.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Evidence curation by iteration. Figure 4 shows that iterative evidence curation (EC) enhances page recall and precision while reducing passages per query compared to the initial hierarchical retrieval (w/o EC). As iterations progress, retrieval performance steadily improves and efficiency increases, demonstrating evidence curation effectiveness.",
                "page": 6,
                "parent_chapter": 88,
                "index": 98,
                "outline": [
                    305.0,
                    682.0,
                    526.0,
                    775.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 7,
                "parent_chapter": 88,
                "index": 99,
                "outline": [
                    69.0,
                    70.5,
                    295.0,
                    180.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 4: Cost-efficiency comparison. Each cell shows the values for the input/output. The asterisk (∗) indicates that, although a much smaller open-source LLM was used, the cost is computed using GPT-4o’s pricing.",
                "title_index": 100,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Method"
                    },
                    "0_1": {
                        "text": "Retrieval"
                    },
                    "0_3": {
                        "text": " Generation"
                    },
                    "1_1": {
                        "text": "Tokens"
                    },
                    "1_2": {
                        "text": " Cost ($)"
                    },
                    "1_3": {
                        "text": " Tokens"
                    },
                    "1_4": {
                        "text": " Cost ($)"
                    },
                    "2_0": {
                        "text": "Dense"
                    },
                    "2_1": {
                        "text": "-"
                    },
                    "2_2": {
                        "text": "-"
                    },
                    "2_3": {
                        "text": "2,666"
                    },
                    "2_4": {
                        "text": "9.3"
                    },
                    "3_3": {
                        "text": "/ 177"
                    },
                    "3_4": {
                        "text": "/ 2.5"
                    },
                    "4_0": {
                        "text": "IRCoT"
                    },
                    "4_1": {
                        "text": "9,610"
                    },
                    "4_2": {
                        "text": "33.4"
                    },
                    "4_3": {
                        "text": "3,475"
                    },
                    "4_4": {
                        "text": "12.1"
                    },
                    "5_1": {
                        "text": "/ 1,372"
                    },
                    "5_2": {
                        "text": "/ 19.1"
                    },
                    "5_3": {
                        "text": "/ 177"
                    },
                    "5_4": {
                        "text": "/ 2.5"
                    },
                    "6_0": {
                        "text": "HiREC"
                    },
                    "6_1": {
                        "text": "4,291"
                    },
                    "6_2": {
                        "text": "∗14.9"
                    },
                    "6_3": {
                        "text": "1,052"
                    },
                    "6_4": {
                        "text": "3.7"
                    },
                    "7_1": {
                        "text": "/ 313"
                    },
                    "7_2": {
                        "text": "/∗4.4"
                    },
                    "7_3": {
                        "text": "/ 159"
                    },
                    "7_4": {
                        "text": "/ 2.2"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            0
                        ],
                        [
                            1,
                            0
                        ]
                    ],
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ]
                    ],
                    [
                        [
                            0,
                            3
                        ],
                        [
                            0,
                            4
                        ]
                    ],
                    [
                        [
                            2,
                            0
                        ],
                        [
                            3,
                            0
                        ]
                    ],
                    [
                        [
                            2,
                            1
                        ],
                        [
                            3,
                            1
                        ]
                    ],
                    [
                        [
                            2,
                            2
                        ],
                        [
                            3,
                            2
                        ]
                    ],
                    [
                        [
                            4,
                            0
                        ],
                        [
                            5,
                            0
                        ]
                    ],
                    [
                        [
                            6,
                            0
                        ],
                        [
                            7,
                            0
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        17.5,
                        35.5,
                        48.5,
                        58.0,
                        68.5,
                        80.5,
                        93.5
                    ],
                    "columns": [
                        41.5,
                        88.5,
                        138.5,
                        185.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 4: Cost-efficiency comparison. Each cell shows the values for the input/output. The asterisk (\\(^*\\)) indicates that, although a much smaller open-source LLM was used, the cost is computed using GPT-4o’s pricing.",
                "page": 7,
                "parent_chapter": 88,
                "index": 100,
                "outline": [
                    70.0,
                    188.5,
                    289.5,
                    235.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Precision-recall. Figure 5 presents the precisionrecall curve, where k varies from 1 to 50 for baseline methods. As expected, increasing k improves recall but lowers precision due to the retrieval of less relevant passages. HiREC achieves both higher precision and recall, consistently exceeding the maximum values observed across all k in the baseline range. This highlights ability of HiREC to retrieve relevant information more effectively while maintaining accuracy.",
                "page": 7,
                "parent_chapter": 88,
                "index": 101,
                "outline": [
                    68.0,
                    243.0,
                    293.0,
                    379.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Cost-efficiency. Table 4 presents the average input/output token counts and total API cost incurred during the retrieval and generation processes. HiREC achieves the highest performance while using significantly fewer tokens and lower cost than the baselines by filtering out irrelevant passages before the answer generation stage. Moreover, compared to IRCoT in retrieval, HiREC uses far fewer tokens, demonstrating that its filtering and complementary question generation enable efficient iteration. Finally, our results show that even a relatively small LLM can effectively perform curation without incurring high costs.",
                "page": 7,
                "parent_chapter": 88,
                "index": 102,
                "outline": [
                    67.0,
                    384.5,
                    293.0,
                    562.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.4 Analysis of Performance Across Various LLM Generators",
                "page": 7,
                "parent_chapter": 66,
                "index": 103,
                "outline": [
                    69.0,
                    569.5,
                    283.0,
                    592.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We thoroughly analyze the effectiveness of the HiREC framework across diverse LLM generator models. For this analysis, additional evaluations were conducted using open-source models such as DeepSeek-R1-Distill-Qwen-14B(Guo et al., 2025) and Qwen-2.5-7B-Instruct(Yang et al., 2024) as generators, in place of GPT-4o.",
                "page": 7,
                "parent_chapter": 103,
                "index": 104,
                "outline": [
                    68.0,
                    600.5,
                    290.0,
                    693.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "As shown in Table 5, HiREC consistently demonstrates superior QA performance across various LLM generators, proving its robustness. Notably, even when employing smaller open-source models, HiREC configurations outperform the Dense baseline. HiREC +Deepseek-14B achieved over 9% higher average answer accuracy compared to Dense+GPT-4o. Another key implication is that integrating sLLMs in the retrieval stage proves to be an effective strategy for reducing overall inference costs while maintaining strong performance.",
                "page": 7,
                "parent_chapter": 103,
                "index": 105,
                "outline": [
                    69.0,
                    695.5,
                    291.0,
                    775.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        105,
                        110
                    ]
                }
            },
            {
                "type": "table",
                "page": 7,
                "parent_chapter": 103,
                "index": 106,
                "outline": [
                    307.5,
                    71.5,
                    521.5,
                    182.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 5: Performance comparison across various LLM generators. Underlined values denote HiREC configura-tions utilizing smaller generators that surpass Dense +GPT-4o performance.",
                "title_index": 107,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Method"
                    },
                    "0_1": {
                        "text": "Numeric\n(Table)"
                    },
                    "0_2": {
                        "text": "Numeric\n(Text)"
                    },
                    "0_3": {
                        "text": "Textual"
                    },
                    "0_4": {
                        "text": " Average"
                    },
                    "1_0": {
                        "text": "Dense"
                    },
                    "1_1": {
                        "text": ""
                    },
                    "1_2": {
                        "text": ""
                    },
                    "1_3": {
                        "text": ""
                    },
                    "1_4": {
                        "text": ""
                    },
                    "2_0": {
                        "text": "+ Qwen-2.5-7B"
                    },
                    "2_1": {
                        "text": "19.76"
                    },
                    "2_2": {
                        "text": "22.34"
                    },
                    "2_3": {
                        "text": "29.52"
                    },
                    "2_4": {
                        "text": "23.87"
                    },
                    "3_0": {
                        "text": "+ Deepseek-14B"
                    },
                    "3_1": {
                        "text": "25.44"
                    },
                    "3_2": {
                        "text": "31.87"
                    },
                    "3_3": {
                        "text": "35.00"
                    },
                    "3_4": {
                        "text": "30.77"
                    },
                    "4_0": {
                        "text": "+ GPT-4o"
                    },
                    "4_1": {
                        "text": "23.69"
                    },
                    "4_2": {
                        "text": "26.18"
                    },
                    "4_3": {
                        "text": "31.00"
                    },
                    "4_4": {
                        "text": "29.22"
                    },
                    "5_0": {
                        "text": "HiREC"
                    },
                    "5_1": {
                        "text": ""
                    },
                    "5_2": {
                        "text": ""
                    },
                    "5_3": {
                        "text": ""
                    },
                    "5_4": {
                        "text": ""
                    },
                    "6_0": {
                        "text": "+ Qwen-2.5-7B"
                    },
                    "6_1": {
                        "text": "27.62"
                    },
                    "6_2": {
                        "text": "33.33"
                    },
                    "6_3": {
                        "text": "36.00"
                    },
                    "6_4": {
                        "text": "32.32"
                    },
                    "7_0": {
                        "text": "+ Deepseek-14B"
                    },
                    "7_1": {
                        "text": "34.39"
                    },
                    "7_2": {
                        "text": "41.39"
                    },
                    "7_3": {
                        "text": "40.53"
                    },
                    "7_4": {
                        "text": "38.76"
                    },
                    "8_0": {
                        "text": "+ GPT-4o"
                    },
                    "8_1": {
                        "text": "37.23"
                    },
                    "8_2": {
                        "text": "48.35"
                    },
                    "8_3": {
                        "text": "41.50"
                    },
                    "8_4": {
                        "text": "42.36"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        23.5,
                        35.0,
                        45.5,
                        55.5,
                        65.0,
                        75.0,
                        85.5,
                        95.5
                    ],
                    "columns": [
                        66.0,
                        105.0,
                        144.0,
                        178.0
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 5: Performance comparison across various LLM generators. Underlined values denote HiREC configurations utilizing smaller generators that surpass Dense +GPT-4o performance.",
                "page": 7,
                "parent_chapter": 103,
                "index": 107,
                "outline": [
                    305.0,
                    189.5,
                    527.0,
                    236.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 7,
                "parent_chapter": 103,
                "index": 108,
                "outline": [
                    308.5,
                    250.5,
                    522.5,
                    347.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 6: Performance results by data source.",
                "title_index": 109,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Method"
                    },
                    "0_1": {
                        "text": " Financebench"
                    },
                    "0_3": {
                        "text": " FinQA"
                    },
                    "0_5": {
                        "text": " SEC-QA"
                    },
                    "1_1": {
                        "text": "Page\nRecall"
                    },
                    "1_2": {
                        "text": "Answer\nAcc"
                    },
                    "1_3": {
                        "text": "Page\nRecall"
                    },
                    "1_4": {
                        "text": "Answer\nAcc"
                    },
                    "1_5": {
                        "text": "Page\nRecall"
                    },
                    "1_6": {
                        "text": "Answer\nAcc"
                    },
                    "2_0": {
                        "text": "Self-RAG"
                    },
                    "2_1": {
                        "text": "9.00"
                    },
                    "2_2": {
                        "text": "13.33"
                    },
                    "2_3": {
                        "text": "22.93"
                    },
                    "2_4": {
                        "text": "3.24"
                    },
                    "2_5": {
                        "text": "4.59"
                    },
                    "2_6": {
                        "text": "4.72"
                    },
                    "3_0": {
                        "text": "RQ-RAG"
                    },
                    "3_1": {
                        "text": "13.67"
                    },
                    "3_2": {
                        "text": "18.67"
                    },
                    "3_3": {
                        "text": "20.32"
                    },
                    "3_4": {
                        "text": "2.88"
                    },
                    "3_5": {
                        "text": "9.38"
                    },
                    "3_6": {
                        "text": "4.72"
                    },
                    "4_0": {
                        "text": "IRCoT"
                    },
                    "4_1": {
                        "text": "9.33"
                    },
                    "4_2": {
                        "text": "20.00"
                    },
                    "4_3": {
                        "text": "32.24"
                    },
                    "4_4": {
                        "text": "22.66"
                    },
                    "4_5": {
                        "text": "4.20"
                    },
                    "4_6": {
                        "text": "7.09"
                    },
                    "5_0": {
                        "text": "HybridSearch"
                    },
                    "5_1": {
                        "text": "10.00"
                    },
                    "5_2": {
                        "text": "29.33"
                    },
                    "5_3": {
                        "text": "29.95"
                    },
                    "5_4": {
                        "text": "23.29"
                    },
                    "5_5": {
                        "text": "10.42"
                    },
                    "5_6": {
                        "text": "7.87"
                    },
                    "6_0": {
                        "text": "HHR"
                    },
                    "6_1": {
                        "text": "16.44"
                    },
                    "6_2": {
                        "text": "34.00"
                    },
                    "6_3": {
                        "text": "41.50"
                    },
                    "6_4": {
                        "text": "29.50"
                    },
                    "6_5": {
                        "text": "10.13"
                    },
                    "6_6": {
                        "text": "5.51"
                    },
                    "7_0": {
                        "text": "Dense"
                    },
                    "7_1": {
                        "text": "26.11"
                    },
                    "7_2": {
                        "text": "33.33"
                    },
                    "7_3": {
                        "text": "40.38"
                    },
                    "7_4": {
                        "text": "27.61"
                    },
                    "7_5": {
                        "text": "15.70"
                    },
                    "7_6": {
                        "text": "9.45"
                    },
                    "8_0": {
                        "text": "HiREC"
                    },
                    "8_1": {
                        "text": "40.00"
                    },
                    "8_2": {
                        "text": "50.00"
                    },
                    "8_3": {
                        "text": "52.49"
                    },
                    "8_4": {
                        "text": "41.61"
                    },
                    "8_5": {
                        "text": "19.97"
                    },
                    "8_6": {
                        "text": "14.17"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            0
                        ],
                        [
                            1,
                            0
                        ]
                    ],
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ]
                    ],
                    [
                        [
                            0,
                            3
                        ],
                        [
                            0,
                            4
                        ]
                    ],
                    [
                        [
                            0,
                            5
                        ],
                        [
                            0,
                            6
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        12.0,
                        33.0,
                        43.5,
                        52.0,
                        60.5,
                        69.5,
                        77.0,
                        86.5
                    ],
                    "columns": [
                        45.5,
                        71.5,
                        101.5,
                        127.5,
                        157.5,
                        184.0
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 6: Performance results by data source.",
                "page": 7,
                "parent_chapter": 103,
                "index": 109,
                "outline": [
                    324.5,
                    356.5,
                    504.5,
                    370.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.5 Performance results by data source",
                "page": 7,
                "parent_chapter": 66,
                "index": 111,
                "outline": [
                    305.0,
                    470.5,
                    494.5,
                    481.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Our analysis of various data sources aims to evaluate benchmark data leakage risk and demonstrate the robustness of the HiREC framework.",
                "page": 7,
                "parent_chapter": 111,
                "index": 112,
                "outline": [
                    305.0,
                    489.0,
                    526.0,
                    526.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "As shown in Table 6, HiREC consistently demonstrates superior performance across all data sources. Notably, all methods, including HiREC , exhibit relatively lower performance on the SECQA subset, as multi-document QA tasks require the synthesis of information from multiple sources for accurate answers. These results suggest the LOFin benchmark is more challenging due to multidocument scenarios and mitigates leakage risk by relying on retrieved evidence.",
                "page": 7,
                "parent_chapter": 111,
                "index": 113,
                "outline": [
                    304.0,
                    528.5,
                    527.0,
                    664.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The final results for the LOFin benchmark, including the expanded SEC-QA subset, can be found in the Appendix.",
                "page": 7,
                "parent_chapter": 111,
                "index": 114,
                "outline": [
                    305.5,
                    666.5,
                    525.0,
                    703.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.6 Comparison of Commercial LLMs with Web Search",
                "page": 7,
                "parent_chapter": 66,
                "index": 115,
                "outline": [
                    305.5,
                    718.0,
                    515.0,
                    741.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Commercial systems such as SearchGPT and Perplexity combine LLMs with web search to answer questions using financial data. Table 8 compares these systems with our approach where Perplexity uses the llama-3.1-sonar-large-128k-online model and SearchGPT uses GPT-4o. In our evaluation based on 40 questions per category, HiREC consistently outperforms these baselines especially on numeric questions, indicating that although commercial systems effectively retrieve relevant documents they often miss the precise numerical details necessary for accurate computation.",
                "page": 7,
                "parent_chapter": 115,
                "index": 116,
                "outline": [
                    305.0,
                    751.0,
                    526.0,
                    775.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        116,
                        121
                    ]
                }
            },
            {
                "type": "table",
                "page": 8,
                "parent_chapter": 115,
                "index": 117,
                "outline": [
                    69.0,
                    69.0,
                    523.5,
                    299.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 7: Case study illustration of HiREC framework effectiveness",
                "title_index": 118,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Step"
                    },
                    "0_1": {
                        "text": "Content"
                    },
                    "1_0": {
                        "text": "Question"
                    },
                    "1_1": {
                        "text": "What isA dobe’sy ear-over-year change in unadjusted operating incomef rom FY2015 to FY2016 (in units ofp ercents\nand round to one decimalp lace)? Give a solution to the question by using the income statement."
                    },
                    "2_0": {
                        "text": "Initial\nRetrieval"
                    },
                    "2_1": {
                        "text": "✓ ADBE_2015_10K (p.59): 3,112,300 3,045,960 Operating income 903,095 412,685 422,723 Non-operating ...\n✗ ADBE_2015_10K (p.39): Net income of $629.6 million increased by $361.2 million, or 135%, during fiscal 2015 ...\n✗ ADBE_2015_10K (p.45): Amortization of purchased intangibles 68.7 ... Total operating expenses $3,148.1 ..."
                    },
                    "3_0": {
                        "text": "Evidence\nCuration"
                    },
                    "3_1": {
                        "text": "Relevant passages: ADBE_2015_10K (p.59) Answerable: False\nExplanation: The context does not provide operating income figures for both FY2015 and FY2016. We only have\noperating income for FY2015 and some intermediate values, but no direct comparison between FY2015 and FY2016.\nComplementary question: What isA dobe’s operating incomef or FY2016?"
                    },
                    "4_0": {
                        "text": "Complementary\nRetrieval"
                    },
                    "4_1": {
                        "text": "✓ ADBE_2016_10K (p.61): 3,148,099 3,112,300 Operating income 1,493,602 903,095 412,685 Non-operating ...\n✗ ADBE_2016_10K (p.35): ITEM 6. SELECTED FINANCIAL DATA ... Net income $1,168,782 $ 629,551 ...\n✗ ADBE_2017_10K (p.57): CONSOLIDATED STATEMENTS OF INCOME ... Operating income 2,168,095 ..."
                    },
                    "5_0": {
                        "text": "Evidence\nCuration"
                    },
                    "5_1": {
                        "text": "Relevant passages: ADBE_2015_10K (p.59), ADBE_2016_10K (p.61) Answerable: True"
                    },
                    "6_0": {
                        "text": "Generation"
                    },
                    "6_1": {
                        "text": "Ground Truth Answer: 46% Generated Answer: 46 Correct: ✓"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        17.8,
                        41.4,
                        85.8,
                        143.2,
                        188.2,
                        213.0
                    ],
                    "columns": [
                        66.9
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 7: Case study illustration of HiREC framework effectiveness",
                "page": 8,
                "parent_chapter": 115,
                "index": 118,
                "outline": [
                    163.5,
                    308.0,
                    431.0,
                    319.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 8,
                "parent_chapter": 115,
                "index": 119,
                "outline": [
                    73.5,
                    325.5,
                    285.5,
                    392.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 8: Answer accuracy for Perplexity, SearchGPT, and HiREC on 40 samples per category.",
                "title_index": 120,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Method"
                    },
                    "0_1": {
                        "text": "Numeric\n(Table)"
                    },
                    "0_2": {
                        "text": "Numeric\n(Text)"
                    },
                    "0_3": {
                        "text": "Textual"
                    },
                    "0_4": {
                        "text": " Average"
                    },
                    "1_0": {
                        "text": "Perplexity"
                    },
                    "1_1": {
                        "text": "2.5"
                    },
                    "1_2": {
                        "text": "5.0"
                    },
                    "1_3": {
                        "text": "37.5"
                    },
                    "1_4": {
                        "text": "15.0"
                    },
                    "2_0": {
                        "text": "SearchGPT"
                    },
                    "2_1": {
                        "text": "15.0"
                    },
                    "2_2": {
                        "text": "32.5"
                    },
                    "2_3": {
                        "text": "35.0"
                    },
                    "2_4": {
                        "text": "27.5"
                    },
                    "3_0": {
                        "text": "HiREC"
                    },
                    "3_1": {
                        "text": "30.0"
                    },
                    "3_2": {
                        "text": "65.0"
                    },
                    "3_3": {
                        "text": "45.0"
                    },
                    "3_4": {
                        "text": "46.7"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        25.0,
                        38.0,
                        49.5
                    ],
                    "columns": [
                        51.5,
                        93.0,
                        134.0,
                        171.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 8: Answer accuracy for Perplexity, SearchGPT, and HiREC on 40 samples per category.",
                "page": 8,
                "parent_chapter": 115,
                "index": 120,
                "outline": [
                    69.0,
                    403.5,
                    290.0,
                    426.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "5.7 Case Study",
                "page": 8,
                "parent_chapter": 66,
                "index": 122,
                "outline": [
                    69.0,
                    581.0,
                    148.0,
                    593.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We analyze how evidence curation filters out unnecessary information and effectively retrieves missing details, and how this impacts the final results. Table 7 presents the retrieval and evidence curation outcomes over iterations for a given question. The initial pass of hierarchical retrieval retrieves passages containing operating income for Adobe for FY2015, but the question remains unanswerable due to missing FY2016 data. The answerability checker detects this gap and triggers the complementary question generator, leading to a complementary pass that retrieves the missing FY2016 operating income. With complete evidence, the answer generator successfully computes the 46% year-over-year change, highlighting the effectiveness of iterative refinement in HiREC.",
                "page": 8,
                "parent_chapter": 122,
                "index": 123,
                "outline": [
                    67.0,
                    599.0,
                    292.0,
                    776.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        123,
                        124
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "6 Conclusion",
                "page": 8,
                "parent_chapter": -1,
                "index": 125,
                "outline": [
                    305.5,
                    380.5,
                    381.0,
                    390.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We introduced HiREC, a retrieval-augmented framework for question answering over standardized financial documents. Our hierarchical retrieval reduces confusion from repeated boilerplate content, while evidence curation filters irrelevant passages and recovers missing information. To evaluate under realistic open-domain conditions, we constructed LOFin, a large-scale benchmark with 1,595 QA pairs across 145,000 SEC filings. The dataset includes multi-document and multihop questions that go beyond prior financial QA benchmarks. Experiments show that HiREC consistently outperforms state-of-the-art baselines in both retrieval quality and answer accuracy, while maintaining cost efficiency through selective passage use. Overall, our findings suggest that HiREC provides a scalable and effective solution for opendomain QA over complex, standardized financial documents.",
                "page": 8,
                "parent_chapter": 125,
                "index": 126,
                "outline": [
                    302.0,
                    400.5,
                    529.0,
                    659.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Limitations",
                "page": 8,
                "parent_chapter": -1,
                "index": 127,
                "outline": [
                    305.0,
                    672.5,
                    364.5,
                    682.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "In our study, we use LLMs for query transformation, evidence curation, and answer generation, making our approach dependent on the performance of the LLMs. We utilize a relatively small LLM, Qwen 2.5 7B (Yang et al., 2024), compared to commercial models.",
                "page": 8,
                "parent_chapter": 127,
                "index": 128,
                "outline": [
                    305.0,
                    695.0,
                    526.0,
                    773.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The benchmarks we employ, FinQA and FinanceBench, are publicly available datasets. Therefore, it is possible that pre-trained large language models have already been exposed to these datasets during their training. In Table 2, GPT-4o (Zeroshot) achieves the second-highest textual performance.",
                "page": 9,
                "parent_chapter": 127,
                "index": 129,
                "outline": [
                    69.0,
                    72.5,
                    292.0,
                    164.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Acknowledgements",
                "page": 9,
                "parent_chapter": -1,
                "index": 130,
                "outline": [
                    69.0,
                    179.5,
                    169.5,
                    192.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "This study was supported by the National R&D Program for Cancer Control through the National Cancer Center (NCC), funded by the Ministry of Health & Welfare, Republic of Korea (RS-2025-02264000).",
                "page": 9,
                "parent_chapter": 130,
                "index": 131,
                "outline": [
                    69.0,
                    202.0,
                    291.0,
                    268.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "This work was also supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. RS-2023-00261068, Development of Lightweight Multimodal AntiPhishing Models and Split-Learning Techniques for Privacy-Preserving Anti-Phishing), and by the Convergence Security Core Talent Training Program (IITP-2024-RS-2024-00423071), supervised by IITP and also funded by the Ministry of Science and ICT (MSIT), Korea.",
                "page": 9,
                "parent_chapter": 130,
                "index": 132,
                "outline": [
                    67.0,
                    269.0,
                    293.0,
                    419.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "References",
                "page": 9,
                "parent_chapter": -1,
                "index": 133,
                "outline": [
                    70.0,
                    444.0,
                    126.0,
                    455.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang. 2023. Hybrid hierarchical retrieval for open-domain question answering. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10680–10689, Toronto, Canada. Association for Computational Linguistics.",
                "page": 9,
                "parent_chapter": 133,
                "index": 134,
                "outline": [
                    68.0,
                    464.0,
                    291.0,
                    542.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511.",
                "page": 9,
                "parent_chapter": 133,
                "index": 135,
                "outline": [
                    70.0,
                    553.5,
                    290.0,
                    598.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Mariam Barry, Gaëtan Caillaut, Pierre Halftermeyer, Raheel Qader, Mehdi Mouayad, Fabrice Le Deit, Dimitri Cariolaro, and Joseph Gesnouin. 2025. Graphrag: Leveraging graph-based efficiency to minimize hallucinations in llm-driven rag for finance data. In Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK), pages 54–65.",
                "page": 9,
                "parent_chapter": 133,
                "index": 136,
                "outline": [
                    69.0,
                    607.0,
                    291.0,
                    686.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610.",
                "page": 9,
                "parent_chapter": 133,
                "index": 137,
                "outline": [
                    70.0,
                    698.0,
                    291.0,
                    740.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts",
                "page": 9,
                "parent_chapter": 133,
                "index": 138,
                "outline": [
                    70.0,
                    753.0,
                    289.5,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588.",
                "page": 9,
                "parent_chapter": 133,
                "index": 139,
                "outline": [
                    315.5,
                    72.5,
                    526.0,
                    103.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Xinyue Chen, Pengyu Gao, Jiangjiang Song, and Xiaoyang Tan. 2024. Hiqa: A hierarchical contextual augmentation rag for massive documents qa. arXiv preprint arXiv:2402.01767.",
                "page": 9,
                "parent_chapter": 133,
                "index": 140,
                "outline": [
                    305.5,
                    118.0,
                    526.0,
                    160.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. 2021. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122.",
                "page": 9,
                "parent_chapter": 133,
                "index": 141,
                "outline": [
                    305.0,
                    172.0,
                    525.0,
                    226.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.",
                "page": 9,
                "parent_chapter": 133,
                "index": 142,
                "outline": [
                    305.0,
                    237.5,
                    526.0,
                    291.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948.",
                "page": 9,
                "parent_chapter": 133,
                "index": 143,
                "outline": [
                    305.5,
                    303.5,
                    526.0,
                    358.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.",
                "page": 9,
                "parent_chapter": 133,
                "index": 144,
                "outline": [
                    305.0,
                    370.0,
                    526.0,
                    412.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944.",
                "page": 9,
                "parent_chapter": 133,
                "index": 145,
                "outline": [
                    305.5,
                    425.0,
                    526.0,
                    466.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Vladimir Karpukhin, Barlas Og˘uz, Sewon Min, Patrick  Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.",
                "page": 9,
                "parent_chapter": 133,
                "index": 146,
                "outline": [
                    305.0,
                    478.5,
                    526.0,
                    531.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Viet Dac Lai, Michael Krumdick, Charles Lovering, Varshini Reddy, Craig Schmidt, and Chris Tanner. 2024. Sec-qa: A systematic evaluation corpus for financial qa. arXiv preprint arXiv:2406.14394.",
                "page": 9,
                "parent_chapter": 133,
                "index": 147,
                "outline": [
                    305.0,
                    544.0,
                    526.0,
                    589.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459–9474.",
                "page": 9,
                "parent_chapter": 133,
                "index": 148,
                "outline": [
                    305.0,
                    598.0,
                    526.0,
                    665.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157–173.",
                "page": 9,
                "parent_chapter": 133,
                "index": 149,
                "outline": [
                    305.0,
                    676.0,
                    525.0,
                    730.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085.",
                "page": 9,
                "parent_chapter": 133,
                "index": 150,
                "outline": [
                    305.5,
                    742.0,
                    525.0,
                    772.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "OpenAI. 2025. searchgpt: A comprehensive guide. https://www.openai.com/searchgpt [Accessed: 2025-02-10].",
                "page": 10,
                "parent_chapter": 133,
                "index": 151,
                "outline": [
                    70.0,
                    72.5,
                    292.0,
                    105.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Perplexity. 2023. Perplexity.ai. https://www. perplexity.ai/. [Large language model].",
                "page": 10,
                "parent_chapter": 133,
                "index": 152,
                "outline": [
                    70.0,
                    114.0,
                    292.0,
                    136.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Nicholas Pipitone and Ghita Houir Alami. 2024. Legalbench-rag: A benchmark for retrievalaugmented generation in the legal domain. arXiv preprint arXiv:2408.10343.",
                "page": 10,
                "parent_chapter": 133,
                "index": 153,
                "outline": [
                    70.0,
                    144.0,
                    291.0,
                    189.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024. Docfinqa: A long-context financial reasoning dataset. arXiv preprint arXiv:2401.06915.",
                "page": 10,
                "parent_chapter": 133,
                "index": 154,
                "outline": [
                    70.0,
                    196.0,
                    292.0,
                    240.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Bhaskarjit Sarmah, Dhagash Mehta, Benika Hall, Rohan Rao, Sunil Patel, and Stefano Pasquali. 2024. Hybridrag: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction. In Proceedings of the 5th ACM International Conference on AI in Finance, pages 608–616.",
                "page": 10,
                "parent_chapter": 133,
                "index": 155,
                "outline": [
                    68.0,
                    247.5,
                    292.0,
                    325.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294.",
                "page": 10,
                "parent_chapter": 133,
                "index": 156,
                "outline": [
                    69.0,
                    333.5,
                    291.0,
                    387.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509.",
                "page": 10,
                "parent_chapter": 133,
                "index": 157,
                "outline": [
                    70.0,
                    397.0,
                    290.0,
                    451.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Jing Wang. 2024. Financerag with hybrid search and reranking.",
                "page": 10,
                "parent_chapter": 133,
                "index": 158,
                "outline": [
                    70.0,
                    460.5,
                    289.5,
                    481.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Multilingual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672.",
                "page": 10,
                "parent_chapter": 133,
                "index": 159,
                "outline": [
                    70.0,
                    490.5,
                    290.0,
                    534.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. 2024b. Rear: A relevance-aware retrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497.",
                "page": 10,
                "parent_chapter": 133,
                "index": 160,
                "outline": [
                    70.0,
                    541.0,
                    291.0,
                    594.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837.",
                "page": 10,
                "parent_chapter": 133,
                "index": 161,
                "outline": [
                    70.0,
                    604.5,
                    290.0,
                    659.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge conflicts for llms: A survey. arXiv preprint arXiv:2403.08319.",
                "page": 10,
                "parent_chapter": 133,
                "index": 162,
                "outline": [
                    70.0,
                    668.0,
                    291.0,
                    712.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Siqiao Xue, Tingting Chen, Fan Zhou, Qingyang Dai, Zhixuan Chu, and Hongyuan Mei. 2024. Famma: A benchmark for financial domain multilingual multimodal question answering. arXiv preprint arXiv:2410.04526.",
                "page": 10,
                "parent_chapter": 133,
                "index": 163,
                "outline": [
                    69.0,
                    719.5,
                    290.0,
                    772.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.",
                "page": 10,
                "parent_chapter": 133,
                "index": 164,
                "outline": [
                    304.0,
                    72.5,
                    526.0,
                    118.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and Renyu Li. 2024. Financial report chunking for effective retrieval augmented generation. arXiv preprint arXiv:2402.05131.",
                "page": 10,
                "parent_chapter": 133,
                "index": 165,
                "outline": [
                    305.0,
                    127.0,
                    525.0,
                    169.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, and Xiao Huang. 2025. A survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958.",
                "page": 10,
                "parent_chapter": 133,
                "index": 166,
                "outline": [
                    304.0,
                    177.5,
                    526.0,
                    243.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2024. DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16103–16120, Bangkok, Thailand. Association for Computational Linguistics.",
                "page": 10,
                "parent_chapter": 133,
                "index": 167,
                "outline": [
                    303.0,
                    251.0,
                    528.0,
                    365.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624.",
                "page": 10,
                "parent_chapter": 133,
                "index": 168,
                "outline": [
                    305.0,
                    372.5,
                    525.0,
                    426.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2024. EfficientRAG: Efficient retriever for multi-hop question answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 3392–3411, Miami, Florida, USA. Association for Computational Linguistics.",
                "page": 10,
                "parent_chapter": 133,
                "index": 169,
                "outline": [
                    303.0,
                    434.0,
                    528.0,
                    524.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Appendix",
                "page": 11,
                "parent_chapter": -1,
                "index": 170,
                "outline": [
                    68.0,
                    70.5,
                    121.5,
                    85.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "A Experiments on the Expanded LOFin-1.6k Benchmark",
                "page": 11,
                "parent_chapter": -1,
                "index": 171,
                "outline": [
                    69.0,
                    93.5,
                    246.5,
                    118.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "A.1 Motivation and Dataset Extension",
                "page": 11,
                "parent_chapter": 171,
                "index": 172,
                "outline": [
                    70.0,
                    128.5,
                    256.0,
                    138.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The initial version of the LOFin-1.4k benchmark was heavily biased toward single-document QA, particularly from FinQA, which limited its coverage of complex financial analysis scenarios. To address this limitation and incorporate reviewer feedback, we expanded the SEC-QA subset from 127 to 333 question-answer pairs by adding 206 new examples. All newly added QA pairs were constructed from recent SEC filings collected up to April 2025, and are designed to require multidocument and multi-page reasoning. The questions include inter-company comparisons, trend analysis, and temporal reasoning. All examples were manually created, which prevents the possibility of data leakage.",
                "page": 11,
                "parent_chapter": 172,
                "index": 173,
                "outline": [
                    67.0,
                    145.0,
                    293.0,
                    349.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 11,
                "parent_chapter": 172,
                "index": 174,
                "outline": [
                    73.5,
                    360.0,
                    286.5,
                    396.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 9: Distribution of expanded LOFin-1.6k QA pairs by dataset source.",
                "title_index": 175,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Dataset Source"
                    },
                    "0_1": {
                        "text": " FinQA"
                    },
                    "0_2": {
                        "text": " SEC-QA"
                    },
                    "0_3": {
                        "text": " Financebench"
                    },
                    "0_4": {
                        "text": " Total"
                    },
                    "1_0": {
                        "text": "# QAs"
                    },
                    "1_1": {
                        "text": "1,112"
                    },
                    "1_2": {
                        "text": "333"
                    },
                    "1_3": {
                        "text": "150"
                    },
                    "1_4": {
                        "text": "1,595"
                    },
                    "2_0": {
                        "text": "Ratio (%)"
                    },
                    "2_1": {
                        "text": "69.72"
                    },
                    "2_2": {
                        "text": "20.88"
                    },
                    "2_3": {
                        "text": "9.40"
                    },
                    "2_4": {
                        "text": "100"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        13.0,
                        25.0
                    ],
                    "columns": [
                        59.0,
                        91.5,
                        130.0,
                        186.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 9: Distribution of expanded LOFin-1.6k QA pairs by dataset source.",
                "page": 11,
                "parent_chapter": 172,
                "index": 175,
                "outline": [
                    69.0,
                    408.0,
                    288.5,
                    431.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "A.1.1 Experiments",
                "page": 11,
                "parent_chapter": 172,
                "index": 176,
                "outline": [
                    70.0,
                    453.0,
                    165.0,
                    464.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "This section reports the results on the expanded benchmark LOFin-1.6k , following the same settings as in Section 5. The evaluation compares our proposed model HiREC against the second-best baseline, Dense. Table 10 presents the main results by answer type. HiREC consistently outperforms Dense across all categories. Table 11 shows performance by data source. Although SEC-QA is a more challenging task, HiREC still demonstrates superior results compared to the baseline. The category distribution by answer type is summarized in Table 13, and the dataset composition by source is reported in Table 9.",
                "page": 11,
                "parent_chapter": 176,
                "index": 177,
                "outline": [
                    67.0,
                    466.5,
                    293.0,
                    645.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "A.1.2 Discussion and Conclusion",
                "page": 11,
                "parent_chapter": 172,
                "index": 178,
                "outline": [
                    69.0,
                    652.5,
                    230.5,
                    662.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The experimental results on the extended SEC-QA subset in expanded LOFin-1.6k validate the robustness and effectiveness of HiREC under realistic multi-document and multi-page QA settings. With this expansion, the LOFin-1.6k benchmark now represents a significantly more challenging and realistic task compared to existing financial QA datasets. Overall, these results demonstrate that HiREC effectively performs evidence-based reasoning and provide strong empirical support that addresses reviewer concerns regarding dataset diversity, integrity, and model generalization.",
                "page": 11,
                "parent_chapter": 178,
                "index": 179,
                "outline": [
                    68.0,
                    667.0,
                    292.0,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        179,
                        180
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "B Supplementary Details on the LOFin Benchmark",
                "page": 11,
                "parent_chapter": -1,
                "index": 181,
                "outline": [
                    305.0,
                    137.0,
                    478.0,
                    161.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "B.1 Categorization Details",
                "page": 11,
                "parent_chapter": 181,
                "index": 182,
                "outline": [
                    305.0,
                    173.0,
                    436.5,
                    184.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Tables 12 and 13 summarize the distribution of QA pairs in LOFin by answer type before and after the SEC-QA expansion. The original dataset is heavily skewed toward numeric-table questions, while the expanded version introduces a larger portion of textual and multi-hop reasoning questions, illustrating the increased complexity and diversity of the benchmark.",
                "page": 11,
                "parent_chapter": 182,
                "index": 183,
                "outline": [
                    304.0,
                    190.5,
                    527.0,
                    296.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "B.2 FinQA Open-Domain Conversion Prompt",
                "page": 11,
                "parent_chapter": 181,
                "index": 184,
                "outline": [
                    305.0,
                    308.0,
                    525.0,
                    321.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "g questions are provided without including the company names and document period. Rewrite the questions to include the given information.",
                "page": 11,
                "parent_chapter": 184,
                "index": 185,
                "outline": [
                    314.0,
                    344.5,
                    516.0,
                    379.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "- Maintain the original meaning of the question while allowing for varied expressions that enable accurate and open-ended responses without altering the factual content.",
                "page": 11,
                "parent_chapter": 184,
                "index": 186,
                "outline": [
                    314.0,
                    396.0,
                    516.0,
                    434.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "- The question must include the company name and the report year.",
                "page": 11,
                "parent_chapter": 184,
                "index": 187,
                "outline": [
                    314.5,
                    440.5,
                    516.0,
                    459.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "- Ticker is used as information about the company name",
                "page": 11,
                "parent_chapter": 184,
                "index": 188,
                "outline": [
                    315.1,
                    466.3,
                    515.2,
                    474.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 11,
                "parent_chapter": 184,
                "index": 189,
                "outline": [
                    314.0,
                    475.0,
                    516.0,
                    627.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 14: This prompt template is used to convert closed-domain FinQA questions into an open-domain format by seamlessly integrating company names and report years into the questions, while preserving their original meaning.",
                "title_index": 190,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "and should not be included in the question."
                    },
                    "1_0": {
                        "text": "- While rephrasing the question, integrate additional\ninformation naturally into the sentence without using\nsimple connectors like ’In’, ’for’, or ’according to’."
                    },
                    "2_0": {
                        "text": "Document information:\n- company ticker: {ticker}\n- document_period: {period}"
                    },
                    "3_0": {
                        "text": "Question: {question}"
                    },
                    "4_0": {
                        "text": "Output format is ##new_question:"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        12.5,
                        53.0,
                        107.5,
                        134.5
                    ],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 14: This prompt template is used to convert closeddomain FinQA questions into an open-domain format by seamlessly integrating company names and report years into the questions, while preserving their original meaning.",
                "page": 11,
                "parent_chapter": 184,
                "index": 190,
                "outline": [
                    304.0,
                    641.5,
                    525.0,
                    699.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "B.3 FinQA Gold Page Selection",
                "page": 11,
                "parent_chapter": 181,
                "index": 191,
                "outline": [
                    305.5,
                    718.5,
                    460.0,
                    729.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Our evidence page annotation for FinQA relies on a dual-method approach. Using FinQA’s qid, we first identify candidate documents and discard any questions associated with documents that have page separation issues. Within each candidate document, we compute BM25 similarity between the FinQA gold table context (which emphasizes distinct numerical values) and each page’s content to select candidate evidence pages. Additionally, an NLI model evaluates the semantic similarity between the concatenated pre_text and post_text (i.e., the context surrounding the table) and the content of each page. If both methods select the same top candidate, that page is accepted and subsequently verified; otherwise, manual annotation is performed to ensure accuracy. Out of 1147 instances, 894 (78%) were automatically accepted. However, among these accepted pages, 9 instances (approximately 1% of 894) contained errors due to OCR issues that merged content from two pages into one. In the remaining 218 instances (19%), the top pages selected by the two methods differed, necessitating manual annotation, and an additional 35 cases (3%) were discarded during preprocessing. This combined approach ensures robust and accurate identification of the correct evidence pages for FinQA.",
                "page": 11,
                "parent_chapter": 191,
                "index": 192,
                "outline": [
                    305.5,
                    737.0,
                    524.5,
                    774.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": true,
                "page_merged_paragraph": {
                    "paragraph_indices": [
                        192,
                        201
                    ]
                }
            },
            {
                "type": "table",
                "page": 12,
                "parent_chapter": 191,
                "index": 193,
                "outline": [
                    73.5,
                    71.5,
                    520.5,
                    122.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 10: Performance by answer type on the expanded LOFin-1.6k benchmark.",
                "title_index": 194,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Model"
                    },
                    "0_1": {
                        "text": " Numeric Table"
                    },
                    "0_4": {
                        "text": " Numeric Text"
                    },
                    "0_7": {
                        "text": " Textual"
                    },
                    "1_1": {
                        "text": "Precision"
                    },
                    "1_2": {
                        "text": " Recall"
                    },
                    "1_3": {
                        "text": " Accuracy"
                    },
                    "1_4": {
                        "text": " Precision"
                    },
                    "1_5": {
                        "text": " Recall"
                    },
                    "1_6": {
                        "text": " Accuracy"
                    },
                    "1_7": {
                        "text": " Precision"
                    },
                    "1_8": {
                        "text": " Recall"
                    },
                    "1_9": {
                        "text": " Accuracy"
                    },
                    "2_0": {
                        "text": "Dense"
                    },
                    "2_1": {
                        "text": "3.47"
                    },
                    "2_2": {
                        "text": "33.88"
                    },
                    "2_3": {
                        "text": "22.92"
                    },
                    "2_4": {
                        "text": "3.77"
                    },
                    "2_5": {
                        "text": "37.73"
                    },
                    "2_6": {
                        "text": "31.50"
                    },
                    "2_7": {
                        "text": "2.90"
                    },
                    "2_8": {
                        "text": "16.18"
                    },
                    "2_9": {
                        "text": "23.93"
                    },
                    "3_0": {
                        "text": "HiREC"
                    },
                    "3_1": {
                        "text": "25.63"
                    },
                    "3_2": {
                        "text": "48.76"
                    },
                    "3_3": {
                        "text": "35.57"
                    },
                    "3_4": {
                        "text": "21.64"
                    },
                    "3_5": {
                        "text": "52.75"
                    },
                    "3_6": {
                        "text": "44.32"
                    },
                    "3_7": {
                        "text": "13.54"
                    },
                    "3_8": {
                        "text": "23.59"
                    },
                    "3_9": {
                        "text": "26.95"
                    },
                    "0_10": {
                        "text": " Average"
                    },
                    "1_10": {
                        "text": " Precision"
                    },
                    "1_11": {
                        "text": " Recall"
                    },
                    "1_12": {
                        "text": " Accuracy"
                    },
                    "2_10": {
                        "text": "3.38"
                    },
                    "2_11": {
                        "text": "29.27"
                    },
                    "2_12": {
                        "text": "26.12"
                    },
                    "3_10": {
                        "text": "20.27"
                    },
                    "3_11": {
                        "text": "41.70"
                    },
                    "3_12": {
                        "text": "35.61"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            0
                        ],
                        [
                            1,
                            0
                        ]
                    ],
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ],
                        [
                            0,
                            3
                        ]
                    ],
                    [
                        [
                            0,
                            4
                        ],
                        [
                            0,
                            5
                        ],
                        [
                            0,
                            6
                        ]
                    ],
                    [
                        [
                            0,
                            7
                        ],
                        [
                            0,
                            8
                        ],
                        [
                            0,
                            9
                        ]
                    ],
                    [
                        [
                            0,
                            10
                        ],
                        [
                            0,
                            11
                        ],
                        [
                            0,
                            12
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        13.4,
                        26.8,
                        38.4
                    ],
                    "columns": [
                        30.3,
                        68.1,
                        97.2,
                        134.4,
                        172.9,
                        201.4,
                        239.2,
                        277.0,
                        306.1,
                        344.0,
                        381.8,
                        410.3
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 10: Performance by answer type on the expanded LOFin-1.6k benchmark.",
                "page": 12,
                "parent_chapter": 191,
                "index": 194,
                "outline": [
                    136.0,
                    132.5,
                    457.0,
                    143.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 12,
                "parent_chapter": 191,
                "index": 195,
                "outline": [
                    72.5,
                    156.0,
                    522.5,
                    208.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 11: Performance by data source on the expanded LOFin-1.6k benchmark.",
                "title_index": 196,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Model"
                    },
                    "0_1": {
                        "text": " FinanceBench"
                    },
                    "0_4": {
                        "text": " FinQA"
                    },
                    "0_7": {
                        "text": " SEC-QA"
                    },
                    "1_1": {
                        "text": "Precision"
                    },
                    "1_2": {
                        "text": " Recall"
                    },
                    "1_3": {
                        "text": " Accuracy"
                    },
                    "1_4": {
                        "text": " Precision"
                    },
                    "1_5": {
                        "text": " Recall"
                    },
                    "1_6": {
                        "text": " Accuracy"
                    },
                    "1_7": {
                        "text": " Precision"
                    },
                    "1_8": {
                        "text": " Recall"
                    },
                    "1_9": {
                        "text": " Accuracy"
                    },
                    "2_0": {
                        "text": "Dense"
                    },
                    "2_1": {
                        "text": "1.80"
                    },
                    "2_2": {
                        "text": "16.00"
                    },
                    "2_3": {
                        "text": "36.00"
                    },
                    "2_4": {
                        "text": "4.00"
                    },
                    "2_5": {
                        "text": "37.28"
                    },
                    "2_6": {
                        "text": "26.44"
                    },
                    "2_7": {
                        "text": "2.88"
                    },
                    "2_8": {
                        "text": "12.66"
                    },
                    "2_9": {
                        "text": "13.51"
                    },
                    "3_0": {
                        "text": "HiREC"
                    },
                    "3_1": {
                        "text": "15.67"
                    },
                    "3_2": {
                        "text": "37.33"
                    },
                    "3_3": {
                        "text": "47.33"
                    },
                    "3_4": {
                        "text": "25.44"
                    },
                    "3_5": {
                        "text": "51.18"
                    },
                    "3_6": {
                        "text": "39.44"
                    },
                    "3_7": {
                        "text": "12.74"
                    },
                    "3_8": {
                        "text": "18.67"
                    },
                    "3_9": {
                        "text": "14.85"
                    },
                    "0_10": {
                        "text": " Average"
                    },
                    "1_10": {
                        "text": " Precision"
                    },
                    "1_11": {
                        "text": " Recall"
                    },
                    "1_12": {
                        "text": " Accuracy"
                    },
                    "2_10": {
                        "text": "2.81"
                    },
                    "2_11": {
                        "text": "21.98"
                    },
                    "2_12": {
                        "text": "25.00"
                    },
                    "3_10": {
                        "text": "17.95"
                    },
                    "3_11": {
                        "text": "35.73"
                    },
                    "3_12": {
                        "text": "33.63"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            0
                        ],
                        [
                            1,
                            0
                        ]
                    ],
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ],
                        [
                            0,
                            3
                        ]
                    ],
                    [
                        [
                            0,
                            4
                        ],
                        [
                            0,
                            5
                        ],
                        [
                            0,
                            6
                        ]
                    ],
                    [
                        [
                            0,
                            7
                        ],
                        [
                            0,
                            8
                        ],
                        [
                            0,
                            9
                        ]
                    ],
                    [
                        [
                            0,
                            10
                        ],
                        [
                            0,
                            11
                        ],
                        [
                            0,
                            12
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        14.6,
                        28.1,
                        39.8
                    ],
                    "columns": [
                        31.6,
                        69.1,
                        97.9,
                        135.9,
                        173.4,
                        202.1,
                        240.2,
                        277.7,
                        307.0,
                        344.5,
                        382.6,
                        411.3
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 11: Performance by data source on the expanded LOFin-1.6k benchmark.",
                "page": 12,
                "parent_chapter": 191,
                "index": 196,
                "outline": [
                    138.0,
                    218.5,
                    456.0,
                    229.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 12,
                "parent_chapter": 191,
                "index": 197,
                "outline": [
                    73.5,
                    250.0,
                    285.5,
                    302.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 12: Distribution of initial LOFin QA pairs by category.",
                "title_index": 198,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Category"
                    },
                    "0_1": {
                        "text": "Numeric\n(Text)"
                    },
                    "0_2": {
                        "text": "Numeric\n(Table)"
                    },
                    "0_3": {
                        "text": "Textual"
                    },
                    "0_4": {
                        "text": " Total"
                    },
                    "1_0": {
                        "text": "# QAs"
                    },
                    "1_1": {
                        "text": "273"
                    },
                    "1_2": {
                        "text": "916"
                    },
                    "1_3": {
                        "text": "200"
                    },
                    "1_4": {
                        "text": "1389"
                    },
                    "2_0": {
                        "text": "Ratio (%)"
                    },
                    "2_1": {
                        "text": "19.65"
                    },
                    "2_2": {
                        "text": "65.95"
                    },
                    "2_3": {
                        "text": "14.40"
                    },
                    "2_4": {
                        "text": "100"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        25.5,
                        38.5
                    ],
                    "columns": [
                        50.0,
                        95.0,
                        140.0,
                        181.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 12: Distribution of initial LOFin QA pairs by category.",
                "page": 12,
                "parent_chapter": 191,
                "index": 198,
                "outline": [
                    70.0,
                    312.5,
                    289.5,
                    335.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 12,
                "parent_chapter": 191,
                "index": 199,
                "outline": [
                    73.5,
                    359.0,
                    285.0,
                    409.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 13: Distribution of expanded LOFin QA pairs by category.",
                "title_index": 200,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Category"
                    },
                    "0_1": {
                        "text": "Numeric\n(Text)"
                    },
                    "0_2": {
                        "text": "Numeric\n(Table)"
                    },
                    "0_3": {
                        "text": "Textual"
                    },
                    "0_4": {
                        "text": " Total"
                    },
                    "1_0": {
                        "text": "# QAs"
                    },
                    "1_1": {
                        "text": "273"
                    },
                    "1_2": {
                        "text": "925"
                    },
                    "1_3": {
                        "text": "397"
                    },
                    "1_4": {
                        "text": "1595"
                    },
                    "2_0": {
                        "text": "Ratio (%)"
                    },
                    "2_1": {
                        "text": "17.11"
                    },
                    "2_2": {
                        "text": "58.00"
                    },
                    "2_3": {
                        "text": "24.89"
                    },
                    "2_4": {
                        "text": "100"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        24.5,
                        37.5
                    ],
                    "columns": [
                        50.0,
                        95.0,
                        140.0,
                        181.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 13: Distribution of expanded LOFin QA pairs by category.",
                "page": 12,
                "parent_chapter": 191,
                "index": 200,
                "outline": [
                    70.0,
                    420.5,
                    289.5,
                    442.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "B.4 Question Templates of SEC-QA",
                "page": 12,
                "parent_chapter": 181,
                "index": 202,
                "outline": [
                    305.5,
                    252.0,
                    480.0,
                    263.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• How much common dividends did {company}pay in the last {num_year} years in US dollars?",
                "page": 12,
                "parent_chapter": 202,
                "index": 203,
                "outline": [
                    317.5,
                    269.0,
                    526.0,
                    306.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• What is the percentage difference of {company1}’s {metric} compared to that of {company2}?",
                "page": 12,
                "parent_chapter": 202,
                "index": 204,
                "outline": [
                    317.5,
                    310.0,
                    526.0,
                    349.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• What is {company}’s overall revenue growth over the last {num_year}-year period?",
                "page": 12,
                "parent_chapter": 202,
                "index": 205,
                "outline": [
                    317.5,
                    350.5,
                    525.0,
                    376.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Among {company_names}, what is the {metric2} of the company that has the highest{metric1}?",
                "page": 12,
                "parent_chapter": 202,
                "index": 206,
                "outline": [
                    317.5,
                    378.0,
                    525.0,
                    416.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "C Experimental Settings",
                "page": 12,
                "parent_chapter": -1,
                "index": 207,
                "outline": [
                    305.0,
                    429.5,
                    440.0,
                    443.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "C.1 Hyperparameters",
                "page": 12,
                "parent_chapter": 207,
                "index": 208,
                "outline": [
                    305.5,
                    453.0,
                    415.5,
                    464.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Candidate document count \\(k_{D}^{\\prime}:100\\)",
                "page": 12,
                "parent_chapter": 208,
                "index": 209,
                "outline": [
                    316.5,
                    469.5,
                    487.0,
                    483.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Final document count \\(k_{D}:\\cdot\\) 5",
                "page": 12,
                "parent_chapter": 208,
                "index": 210,
                "outline": [
                    316.5,
                    483.0,
                    455.5,
                    497.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Final passage count \\(k_{P}:5\\)",
                "page": 12,
                "parent_chapter": 208,
                "index": 211,
                "outline": [
                    317.5,
                    497.0,
                    444.5,
                    510.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Maximum relevant passages \\(k_{P}^{\\prime}\\): 10",
                "page": 12,
                "parent_chapter": 208,
                "index": 212,
                "outline": [
                    317.5,
                    510.0,
                    485.5,
                    525.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Maximum iterations \\(i_{m a x}:3\\)",
                "page": 12,
                "parent_chapter": 208,
                "index": 213,
                "outline": [
                    317.5,
                    525.0,
                    455.5,
                    537.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• LLM temperature: 0.01",
                "page": 12,
                "parent_chapter": 208,
                "index": 214,
                "outline": [
                    316.5,
                    537.5,
                    431.5,
                    551.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Chunking tool: Langchain5’s RecursiveCharacterTextSplitter",
                "page": 12,
                "parent_chapter": 208,
                "index": 215,
                "outline": [
                    317.5,
                    551.0,
                    526.0,
                    576.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Chunk size: 1024",
                "page": 12,
                "parent_chapter": 208,
                "index": 216,
                "outline": [
                    317.5,
                    578.0,
                    404.5,
                    589.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "• Overlap between chunks: 30",
                "page": 12,
                "parent_chapter": 208,
                "index": 217,
                "outline": [
                    318.5,
                    592.0,
                    453.5,
                    604.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "C.2 Baselines",
                "page": 12,
                "parent_chapter": 207,
                "index": 218,
                "outline": [
                    305.0,
                    618.0,
                    375.5,
                    628.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Perplexity: The Perplexity baseline employs the llama-3.1-sonar-large-128k-online model, integrating web search to supplement the context used for answer generation.",
                "page": 12,
                "parent_chapter": 218,
                "index": 219,
                "outline": [
                    305.0,
                    636.0,
                    526.0,
                    688.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Self-RAG: Self-RAG utilizes a 13B model in a Long-form setting. It leverages the Contriever model to retrieve 10 context passages for each query, ensuring comprehensive coverage of the relevant information.",
                "page": 12,
                "parent_chapter": 218,
                "index": 220,
                "outline": [
                    304.0,
                    689.5,
                    526.0,
                    755.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "RQ-RAG: text-embedding-3-small is used in RQ-RAG to retrieve three context passages at each retrieval step. Configured for multi-hop QA with an exploration depth of two, it employs iterative retrieval to continuously refine the contextual information used for answer generation.",
                "page": 13,
                "parent_chapter": 218,
                "index": 222,
                "outline": [
                    69.0,
                    72.5,
                    291.0,
                    152.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "HybridSearch: HybridSearch combines BM25-based scores with dense scores computed using the text-embedding-3-small model. This hybrid approach adheres to its original configuration to effectively balance lexical and semantic matching.",
                "page": 13,
                "parent_chapter": 218,
                "index": 223,
                "outline": [
                    69.0,
                    155.0,
                    291.0,
                    221.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "IRCoT: IRCoT relies on a BM25-based retriever and employs GPT-4o for chain-of-thought (CoT) reasoning. It performs iterative retrieval by executing up to 3 iterations, with 5 passages being retrieved during each iteration.",
                "page": 13,
                "parent_chapter": 218,
                "index": 224,
                "outline": [
                    69.0,
                    223.0,
                    290.0,
                    289.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "HHR: HHR implements a hybrid retrieval strategy that combines dense and sparse retrieval methods. In this approach, the text-embedding-3-small model is used for dense retrieval, and the system first retrieves the top 5 documents; within each document, the top 10 passages are then selected.",
                "page": 13,
                "parent_chapter": 218,
                "index": 225,
                "outline": [
                    69.0,
                    291.0,
                    290.0,
                    385.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Dense: The Dense method uses the text-embedding-3-small model to initially retrieve the top 50 relevant passages. These passages are subsequently reranked using a DeBERTa-v3 model to determine their final order.",
                "page": 13,
                "parent_chapter": 218,
                "index": 226,
                "outline": [
                    69.0,
                    387.0,
                    290.0,
                    451.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "C.3 Textual Evaluation Prompt",
                "page": 13,
                "parent_chapter": 207,
                "index": 227,
                "outline": [
                    70.0,
                    466.5,
                    225.0,
                    477.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "This prompt is used to evaluate the factual correctness of generated textual answers by instructing the LLM to compare them against the ground truth within the provided context.",
                "page": 13,
                "parent_chapter": 227,
                "index": 228,
                "outline": [
                    69.0,
                    486.0,
                    291.0,
                    538.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 13,
                "parent_chapter": 227,
                "index": 229,
                "outline": [
                    74.5,
                    554.5,
                    282.0,
                    748.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 15: GPT-4o LM Evaluation Prompt Template",
                "title_index": 230,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "You are a highly knowledgeable expert and teacher in\nthe finance domain."
                    },
                    "1_0": {
                        "text": "You are reviewing a student’s answers to financial\nquestions."
                    },
                    "2_0": {
                        "text": "You are given the context, the question, the student’s\nanswer and the student’s explanation and the ground -\ntruth answer."
                    },
                    "3_0": {
                        "text": "Please use the given information and refer to the ground -\ntruth answer to determine if the student’s answer is\ncorrect."
                    },
                    "4_0": {
                        "text": "The input information is as follows:"
                    },
                    "5_0": {
                        "text": "context: ’{gold context}’"
                    },
                    "6_0": {
                        "text": "question: ’{question}’"
                    },
                    "7_0": {
                        "text": "ground-truth answer: ’{answer}’"
                    },
                    "8_0": {
                        "text": "student’s answer: ’{generated answer}’"
                    },
                    "9_0": {
                        "text": "Please respond directly as either ’correct’ or ’incorrect’."
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        27.0,
                        50.0,
                        81.0,
                        113.0,
                        126.0,
                        138.0,
                        150.0,
                        162.0,
                        174.0
                    ],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 15: GPT-4o LM Evaluation Prompt Template",
                "page": 13,
                "parent_chapter": 227,
                "index": 230,
                "outline": [
                    76.0,
                    760.5,
                    283.0,
                    771.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "D LLM Prompts",
                "page": 13,
                "parent_chapter": -1,
                "index": 231,
                "outline": [
                    305.0,
                    71.5,
                    401.0,
                    84.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "This section presents a detailed overview of the prompting strategies employed in our framework. Our prompt templates are designed based on common instruction formats introduced in the Prompt Engineering Guide6, and were initially generated using GPT-4o in an instruction-following mode. To ensure consistency with each module’s output format, we manually refined the initial prompts through a lightweight post-editing process.",
                "page": 13,
                "parent_chapter": 231,
                "index": 232,
                "outline": [
                    303.0,
                    92.5,
                    528.0,
                    215.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "No explicit constraints were imposed during prompt construction. We did not apply fewshot prompting due to practical limitations such as prompt length and inference cost. The final prompts are tailored to the specific needs of each module—query rewriting, document summarization, evidence curation, and answer generation—and are provided in this section.",
                "page": 13,
                "parent_chapter": 231,
                "index": 233,
                "outline": [
                    303.0,
                    215.0,
                    527.0,
                    322.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "These prompt designs play a central role in facilitating efficient information processing and enabling accurate responses to complex financial questions. By disclosing our prompting approach in detail, we aim to enhance the reproducibility and transparency of our overall system.",
                "page": 13,
                "parent_chapter": 231,
                "index": 234,
                "outline": [
                    305.0,
                    323.5,
                    526.0,
                    403.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "D.1 Summarization Prompt",
                "page": 13,
                "parent_chapter": 231,
                "index": 235,
                "outline": [
                    305.0,
                    413.5,
                    443.5,
                    426.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "To assist document indexing during hierarchical retrieval (section 4.1.1), we use a concise LLMbased summarization prompt as follows Table 16:",
                "page": 13,
                "parent_chapter": 235,
                "index": 236,
                "outline": [
                    304.0,
                    430.5,
                    526.0,
                    469.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 13,
                "parent_chapter": 235,
                "index": 237,
                "outline": [
                    314.5,
                    486.0,
                    514.5,
                    513.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 16: Summarization prompt",
                "title_index": 238,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "You are a helpful assistant. Summarize the following\ntext:"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 16: Summarization prompt",
                "page": 13,
                "parent_chapter": 235,
                "index": 238,
                "outline": [
                    347.5,
                    534.0,
                    480.5,
                    544.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "D.2 Query Transformation Prompt",
                "page": 14,
                "parent_chapter": 231,
                "index": 240,
                "outline": [
                    70.0,
                    72.5,
                    241.5,
                    83.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "To reduce noise from overly specific or ambiguous financial terms in user queries, we rewrite them into focused retrieval queries using the prompt below Table 17:",
                "page": 14,
                "parent_chapter": 240,
                "index": 241,
                "outline": [
                    69.0,
                    161.5,
                    290.0,
                    211.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 14,
                "parent_chapter": 240,
                "index": 242,
                "outline": [
                    74.5,
                    299.0,
                    282.0,
                    486.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 17: Financial query transformation instructions",
                "title_index": 243,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "You are an AI that rewrites user questions about finan-\ncial topics into concise meta-focused queries."
                    },
                    "1_0": {
                        "text": "1) Identify the key financial terms or metrics in the\nquestion."
                    },
                    "2_0": {
                        "text": "2) Determine which type of documents typically contain\nthose terms."
                    },
                    "3_0": {
                        "text": "3) Transform the user’s question into a short query refer-\nencing the financial terms and the relevant documents.\n4) Do not reveal the transformation process or provide\nexamples."
                    },
                    "4_0": {
                        "text": "5) Output only the final rewritten query."
                    },
                    "5_0": {
                        "text": "## Question: {Question}"
                    },
                    "6_0": {
                        "text": "### Output format"
                    },
                    "7_0": {
                        "text": "## Query: {Rewritten query}"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        32.5,
                        57.5,
                        76.5,
                        116.5,
                        131.5,
                        151.5,
                        166.5
                    ],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 17: Financial query transformation instructions",
                "page": 14,
                "parent_chapter": 240,
                "index": 243,
                "outline": [
                    72.5,
                    496.5,
                    286.5,
                    507.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "D.3 Evidence Curation Prompt",
                "page": 14,
                "parent_chapter": 231,
                "index": 244,
                "outline": [
                    70.0,
                    634.5,
                    224.0,
                    645.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "The evidence curation module jointly performs three tasks—passage filtering, answerability checking, and complementary question generation—using a single prompt as follows Table 18:",
                "page": 14,
                "parent_chapter": 244,
                "index": 245,
                "outline": [
                    69.0,
                    722.5,
                    291.0,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 14,
                "parent_chapter": 244,
                "index": 246,
                "outline": [
                    311.0,
                    72.5,
                    520.5,
                    469.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 18: Evidence curation instructions",
                "title_index": 247,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "### Instruction"
                    },
                    "1_0": {
                        "text": "You are a financial expert. Evaluate the provided context\nto determine if it contains enough information to answer\nthe given question."
                    },
                    "2_0": {
                        "text": "1. Read the context carefully and decide if it contains\nenough information to answer the question."
                    },
                    "3_0": {
                        "text": "2. If it is answerable, set ’is_answerable: answerable’\nand provide the answer in ’answer’."
                    },
                    "4_0": {
                        "text": "3. If it is not answerable, set ’is_answerable: unanswer-\nable’. Then:"
                    },
                    "5_0": {
                        "text": "- List the relevant document IDs in ’answer-\nable_doc_ids’ in order of relevance (from most to least\nrelevant)."
                    },
                    "6_0": {
                        "text": "- Explain what specific information is missing in ’miss-\ning_information’."
                    },
                    "7_0": {
                        "text": "- Provide a concise question in ’refined_query’ to search\nfor exactly that missing information."
                    },
                    "8_0": {
                        "text": "4. Output your result strictly in the specified format\nbelow using ’##’ headers."
                    },
                    "9_0": {
                        "text": "###Inputs"
                    },
                    "10_0": {
                        "text": "Context:"
                    },
                    "11_0": {
                        "text": "Context1 (ID: 1): Title is {title1}. Content is {content1}"
                    },
                    "12_0": {
                        "text": "Context2 (ID: 2): Title is {title2}. Content is {content2}"
                    },
                    "13_0": {
                        "text": "Question: {question}"
                    },
                    "14_0": {
                        "text": "### Output format"
                    },
                    "15_0": {
                        "text": "## is_answerable: answerable or unanswerable"
                    },
                    "16_0": {
                        "text": "## missing_information: If ’unanswerable’, specify the\ndetails or data needed; if ’answerable’, None\n## answer: If ’answerable’, provide the answer; if ’unan-\nswerable’, then None\n## answerable_doc_ids: Provide a list of document IDs\nthat contain relevant information (e.g., [1, 2]). If none,\nuse []\n## refined_query: If ’unanswerable’, provide a refined\nquestion to obtain the missing information;"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        17.1,
                        48.1,
                        68.2,
                        87.9,
                        107.5,
                        137.0,
                        157.1,
                        176.8,
                        202.1,
                        217.1,
                        226.4,
                        237.3,
                        252.3,
                        272.4,
                        286.9,
                        296.7
                    ],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 18: Evidence curation instructions",
                "page": 14,
                "parent_chapter": 244,
                "index": 247,
                "outline": [
                    333.0,
                    479.5,
                    496.0,
                    489.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "D.4 Generation Prompt",
                "page": 14,
                "parent_chapter": 231,
                "index": 248,
                "outline": [
                    304.0,
                    609.0,
                    424.5,
                    622.5
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "We adopt two generation styles depending on the question type. For numerical reasoning tasks, we use Program-of-Thoughts (PoT) prompting to generate Python code (see Table 19); for textual reasoning, we apply Chain-of-Thought (CoT) prompting (see Table 20).",
                "page": 14,
                "parent_chapter": 248,
                "index": 249,
                "outline": [
                    304.0,
                    695.0,
                    526.0,
                    775.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 15,
                "parent_chapter": 248,
                "index": 250,
                "outline": [
                    310.0,
                    78.0,
                    516.0,
                    262.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 20: Chain-of-Thought prompt for generating a Python program to answer a financial question.",
                "title_index": 253,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "[System Input]"
                    },
                    "1_0": {
                        "text": "You are a financial expert, and you are supposed to an-\nswer the given question based on the provided financial\ndocument context. You need to first think through the\nproblem step by step, documenting each necessary step.\nThen, you are required to conclude your response with\nthe final answer in your last sentence as \"Therefore, the\nanswer is final answer."
                    },
                    "2_0": {
                        "text": "[User Input]"
                    },
                    "3_0": {
                        "text": "Context:"
                    },
                    "4_0": {
                        "text": "Sources: {title1} - {content1}"
                    },
                    "5_0": {
                        "text": "Sources: {title2} - {content2}"
                    },
                    "6_0": {
                        "text": "Question: {question}"
                    },
                    "7_0": {
                        "text": "Let’s think step by step to answer the given question."
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        12.5,
                        83.5,
                        96.5,
                        105.5,
                        116.5,
                        136.5,
                        161.5
                    ],
                    "columns": []
                }
            },
            {
                "type": "table",
                "page": 15,
                "parent_chapter": 248,
                "index": 251,
                "outline": [
                    74.5,
                    88.0,
                    285.0,
                    618.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 19: The prompt for generating a Python program to answer a financial question.",
                "title_index": 252,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "You are a financial expert, you are supposed to generate\na Python program to answer the given question based on\nthe provided financial document context. The returned\nvalue of the program is supposed to be the answer."
                    },
                    "1_0": {
                        "text": "ˋˋˋ\npython\ndef solution():"
                    },
                    "2_0": {
                        "text": "# Define variables name and value based on\nthe given context\nguarantees = 210\ntotal_exposure = 716"
                    },
                    "3_0": {
                        "text": "# Do math calculation to get the answer\nanswer = (guarantees / total_exposure) *\n100"
                    },
                    "4_0": {
                        "text": "# return answer\nreturn answer\nˋˋˋ"
                    },
                    "5_0": {
                        "text": "[User Input]"
                    },
                    "6_0": {
                        "text": "Context:"
                    },
                    "7_0": {
                        "text": "Sources: {title1} - {content1}\nSources: {title2} - {content2}"
                    },
                    "8_0": {
                        "text": "Question: {question}"
                    },
                    "9_0": {
                        "text": "Please generate a Python program to answer the given\nquestion. The format of the program should be the\nfollowing:"
                    },
                    "10_0": {
                        "text": "ˋˋˋ\npython"
                    },
                    "11_0": {
                        "text": "def solution():"
                    },
                    "12_0": {
                        "text": "# Define variables name and value based on\nthe given context"
                    },
                    "13_0": {
                        "text": "# Do math calculation to get the answer"
                    },
                    "14_0": {
                        "text": "# return answer"
                    },
                    "15_0": {
                        "text": "return answer\nˋˋˋ"
                    },
                    "16_0": {
                        "text": "Continue the program to answer the question. The re-"
                    },
                    "17_0": {
                        "text": "turned value of the program is supposed to be the an-\nswer:"
                    },
                    "18_0": {
                        "text": "ˋˋˋ\npython"
                    },
                    "19_0": {
                        "text": "def solution():"
                    },
                    "20_0": {
                        "text": "# Define variables name and value based on\nthe given context\nˋˋˋ"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        46.9,
                        71.8,
                        116.6,
                        156.0,
                        193.9,
                        211.9,
                        220.8,
                        251.2,
                        276.7,
                        315.4,
                        330.6,
                        340.2,
                        365.8,
                        385.8,
                        400.3,
                        423.0,
                        440.3,
                        463.8,
                        480.3,
                        490.0
                    ],
                    "columns": []
                }
            },
            {
                "type": "paragraph",
                "text": "Table 19: The prompt for generating a Python program to answer a financial question.",
                "page": 15,
                "parent_chapter": 248,
                "index": 252,
                "outline": [
                    70.0,
                    630.0,
                    289.5,
                    650.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Table 20: Chain-of-Thought prompt for generating a Python program to answer a financial question.",
                "page": 15,
                "parent_chapter": 248,
                "index": 253,
                "outline": [
                    305.0,
                    275.5,
                    525.0,
                    297.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "E Generalization to Other Domains",
                "page": 15,
                "parent_chapter": -1,
                "index": 254,
                "outline": [
                    305.0,
                    318.0,
                    496.0,
                    328.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "To evaluate the effectiveness of our method beyond the financial domain, we assess retrieval performance on the LegalBench-RAG (Pipitone and Alami, 2024) dataset, a legal-domain benchmark where retrieval is particularly challenging due to domain-specific terminology and structured documents. Using a mini question set, we conduct retrieval over the full document corpus to simulate realistic conditions.",
                "page": 15,
                "parent_chapter": 254,
                "index": 255,
                "outline": [
                    304.0,
                    341.0,
                    527.0,
                    460.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "As shown in Table 21, HiREC achieves the highest Precision@k (41.42) and Recall@k (48.50) while using the lowest average number of retrieved passages per query (\\(k\\) = 3.2). This result highlights the framework’s ability to efficiently retrieve essential content with minimal redundancy. In particular, HiREC demonstrates strong performance on structured datasets such as MAUD and CUAD, outperforming other models by a large margin in both P@k and R@k.",
                "page": 15,
                "parent_chapter": 254,
                "index": 256,
                "outline": [
                    303.0,
                    463.0,
                    528.0,
                    597.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "ContractNLI is a document-level natural language inference task in which each contract is evaluated against a fixed set of hypotheses. Unlike passage-based QA tasks, it requires global reasoning over the full document. As a result, Dense may benefit from broad recall, potentially retrieving passages that touch on relevant hypotheses by chance. In contrast, HiREC follows a structured evidence curation pipeline that emphasizes precision, often selecting fewer but more targeted documents. This structural difference helps explain the smaller performance gap between HiREC and Dense on ContractNLI compared to other tasks.",
                "page": 15,
                "parent_chapter": 254,
                "index": 257,
                "outline": [
                    303.0,
                    599.0,
                    528.0,
                    776.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 16,
                "parent_chapter": 254,
                "index": 258,
                "outline": [
                    72.5,
                    71.5,
                    521.5,
                    161.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 21: Average Precision and Recall for the LegalBench-RAG dataset. The best performance values are highlighted in bold and the second-best are underlined.",
                "title_index": 259,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Model"
                    },
                    "0_1": {
                        "text": "PrivacyQA"
                    },
                    "0_3": {
                        "text": " ContractNLI"
                    },
                    "0_5": {
                        "text": " MAUD"
                    },
                    "0_7": {
                        "text": " CUAD"
                    },
                    "0_9": {
                        "text": " Average"
                    },
                    "1_1": {
                        "text": "P@k"
                    },
                    "1_2": {
                        "text": " R@k"
                    },
                    "1_3": {
                        "text": " P@k"
                    },
                    "1_4": {
                        "text": " R@k"
                    },
                    "1_5": {
                        "text": " P@k"
                    },
                    "1_6": {
                        "text": " R@k"
                    },
                    "1_7": {
                        "text": " P@k"
                    },
                    "1_8": {
                        "text": " R@k"
                    },
                    "1_9": {
                        "text": " P@k"
                    },
                    "2_0": {
                        "text": "Hybrid"
                    },
                    "2_1": {
                        "text": "21.13"
                    },
                    "2_2": {
                        "text": "35.99"
                    },
                    "2_3": {
                        "text": "12.06"
                    },
                    "2_4": {
                        "text": "34.36"
                    },
                    "2_5": {
                        "text": "1.34"
                    },
                    "2_6": {
                        "text": "2.05"
                    },
                    "2_7": {
                        "text": "7.73"
                    },
                    "2_8": {
                        "text": "21.97"
                    },
                    "2_9": {
                        "text": "10.57"
                    },
                    "3_0": {
                        "text": "HHR"
                    },
                    "3_1": {
                        "text": "20.31"
                    },
                    "3_2": {
                        "text": "35.57"
                    },
                    "3_3": {
                        "text": "19.28"
                    },
                    "3_4": {
                        "text": "54.25"
                    },
                    "3_5": {
                        "text": "8.04"
                    },
                    "3_6": {
                        "text": "14.20"
                    },
                    "3_7": {
                        "text": "8.66"
                    },
                    "3_8": {
                        "text": "25.49"
                    },
                    "3_9": {
                        "text": "14.07"
                    },
                    "4_0": {
                        "text": "Dense"
                    },
                    "4_1": {
                        "text": "31.55"
                    },
                    "4_2": {
                        "text": "52.21"
                    },
                    "4_3": {
                        "text": "28.56"
                    },
                    "4_4": {
                        "text": "78.69"
                    },
                    "4_5": {
                        "text": "11.03"
                    },
                    "4_6": {
                        "text": "20.87"
                    },
                    "4_7": {
                        "text": "10.82"
                    },
                    "4_8": {
                        "text": "29.92"
                    },
                    "4_9": {
                        "text": "20.49"
                    },
                    "5_0": {
                        "text": "HiREC"
                    },
                    "5_1": {
                        "text": "55.79"
                    },
                    "5_2": {
                        "text": "49.94"
                    },
                    "5_3": {
                        "text": "35.46"
                    },
                    "5_4": {
                        "text": "53.31"
                    },
                    "5_5": {
                        "text": "33.70"
                    },
                    "5_6": {
                        "text": "36.80"
                    },
                    "5_7": {
                        "text": "40.74"
                    },
                    "5_8": {
                        "text": "53.95"
                    },
                    "5_9": {
                        "text": "41.42"
                    },
                    "0_11": {
                        "text": "Avg. k"
                    },
                    "1_10": {
                        "text": " R@k"
                    },
                    "2_10": {
                        "text": "23.59"
                    },
                    "2_11": {
                        "text": "5.0"
                    },
                    "3_10": {
                        "text": "32.38"
                    },
                    "3_11": {
                        "text": "5.0"
                    },
                    "4_10": {
                        "text": "45.42"
                    },
                    "4_11": {
                        "text": "5.0"
                    },
                    "5_10": {
                        "text": "48.50"
                    },
                    "5_11": {
                        "text": "3.2"
                    }
                },
                "merged": [
                    [
                        [
                            0,
                            0
                        ],
                        [
                            1,
                            0
                        ]
                    ],
                    [
                        [
                            0,
                            1
                        ],
                        [
                            0,
                            2
                        ]
                    ],
                    [
                        [
                            0,
                            3
                        ],
                        [
                            0,
                            4
                        ]
                    ],
                    [
                        [
                            0,
                            5
                        ],
                        [
                            0,
                            6
                        ]
                    ],
                    [
                        [
                            0,
                            7
                        ],
                        [
                            0,
                            8
                        ]
                    ],
                    [
                        [
                            0,
                            9
                        ],
                        [
                            0,
                            10
                        ]
                    ],
                    [
                        [
                            0,
                            11
                        ],
                        [
                            1,
                            11
                        ]
                    ]
                ],
                "grid": {
                    "rows": [
                        17.5,
                        33.9,
                        50.3,
                        60.2,
                        71.3
                    ],
                    "columns": [
                        43.8,
                        80.1,
                        116.3,
                        153.2,
                        189.4,
                        225.7,
                        261.9,
                        298.2,
                        335.0,
                        371.2,
                        407.5
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 21: Average Precision and Recall for the LegalBench-RAG dataset. The best performance values are highlighted in bold and the second-best are underlined.",
                "page": 16,
                "parent_chapter": 254,
                "index": 259,
                "outline": [
                    70.0,
                    168.5,
                    524.5,
                    191.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Overall, these results confirm the generalizability of HiREC to non-financial domains, demonstrating robustness across diverse legal document types and retrieval tasks.",
                "page": 16,
                "parent_chapter": 254,
                "index": 260,
                "outline": [
                    69.0,
                    214.0,
                    291.0,
                    263.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "F Failure Case Study",
                "page": 16,
                "parent_chapter": -1,
                "index": 261,
                "outline": [
                    70.0,
                    278.0,
                    188.0,
                    291.0
                ],
                "is_chapter_title": true,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Retrieval Failure Case In this case, the model fails to retrieve all relevant documents across multiple fiscal years. The question explicitly asks for the total common dividends paid over the past three years (as of 2023), which requires retrieving information from multiple annual filings. However, the model only retrieves passages from the 2023 report. This indicates difficulty in reasoning over temporal scope and aggregating evidence when relevant information is distributed across documents.",
                "page": 16,
                "parent_chapter": 261,
                "index": 262,
                "outline": [
                    68.0,
                    299.0,
                    292.0,
                    432.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "paragraph",
                "text": "Generation Failure Case In this case, the model retrieves a passage containing sufficient information to answer the question. However, the generated answer is incorrect due to faulty arithmetic reasoning. Specifically, the free cash flow should be computed as the difference between cash from operations and capital expenditures. Although both values are retrieved, the model produces an incorrect value, indicating limitations in table-grounded numerical reasoning and code execution.",
                "page": 16,
                "parent_chapter": 261,
                "index": 263,
                "outline": [
                    68.0,
                    441.5,
                    292.0,
                    577.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 17,
                "parent_chapter": 261,
                "index": 264,
                "outline": [
                    72.5,
                    152.5,
                    519.5,
                    294.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 22: Retrieval failure: model fails to retrieve documents from multiple years required to answer a multi-year aggregation question.",
                "title_index": 265,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Step"
                    },
                    "0_1": {
                        "text": "Content"
                    },
                    "1_0": {
                        "text": "Question"
                    },
                    "1_1": {
                        "text": "How much common dividends didB ank ofA mericap ay in the last 3y ears, as of 2023, in millions of\nUS dollars?"
                    },
                    "2_0": {
                        "text": "Gold\nEvidences"
                    },
                    "2_1": {
                        "text": "BAC_2023_10K (p.94), BAC_2022_10K (p.94), BAC_2021_10K (p.94)"
                    },
                    "3_0": {
                        "text": "Retrieval"
                    },
                    "3_1": {
                        "text": "✗ BAC_2023_10K (p.141): NOTE 13 Shareholders’ Equity\nCommon Stock . . .\n✗ BAC_2023_10K (p.150): Defined Contribution Plans\nThe Corporation. . ."
                    },
                    "4_0": {
                        "text": "Generation"
                    },
                    "4_1": {
                        "text": "Ground Truth Answer: 25,718 Generated Answer: 2,560"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        15.1,
                        41.3,
                        68.1,
                        122.8
                    ],
                    "columns": [
                        59.9
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 22: Retrieval failure: model fails to retrieve documents from multiple years required to answer a multi-year aggregation question.",
                "page": 17,
                "parent_chapter": 261,
                "index": 265,
                "outline": [
                    70.0,
                    303.5,
                    525.0,
                    326.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 17,
                "parent_chapter": 261,
                "index": 266,
                "outline": [
                    70.0,
                    493.0,
                    520.5,
                    660.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 23: Generation failure: despite correct retrieval, the model fails to compute free cash flow accurately from table values.",
                "title_index": 267,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Step"
                    },
                    "0_1": {
                        "text": "Content"
                    },
                    "1_0": {
                        "text": "Question"
                    },
                    "1_1": {
                        "text": "According to the informationp rovided in the statement of cashfl ows, what is the FY2020f ree cash\nflow (FCF)f or GeneralM ills?A nswer in USD millions."
                    },
                    "2_0": {
                        "text": "Gold\nEvidences"
                    },
                    "2_1": {
                        "text": "GIS_2020_10K (p.51)"
                    },
                    "3_0": {
                        "text": "Retrieval"
                    },
                    "3_1": {
                        "text": "✓ GIS_2020_10K (p.51): . . . $3,676.2. . . Capital expenditures $460.8 . . .\n✗ GIS_2020_10K (p.17): Adjusted diluted EPS of $3.61 increased 12 percent on a constant-currency\nbasis . . .\n✗ GIS_2020_10K (p.36): Free cash flow $3,215.4 . . . Net cash provided by operating activities\nconversion . . ."
                    },
                    "4_0": {
                        "text": "LLM Output"
                    },
                    "4_1": {
                        "text": "cash_from_operations = 3700 # in USD millions\ncapex = 461 # in USD millions"
                    },
                    "5_0": {
                        "text": "Generation"
                    },
                    "5_1": {
                        "text": "Ground Truth Answer: 3,215 Generated Answer: 3,239"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        16.4,
                        41.6,
                        65.7,
                        124.9,
                        150.8
                    ],
                    "columns": [
                        66.3
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 23: Generation failure: despite correct retrieval, the model fails to compute free cash flow accurately from table values.",
                "page": 17,
                "parent_chapter": 261,
                "index": 267,
                "outline": [
                    70.0,
                    669.0,
                    524.5,
                    689.0
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            },
            {
                "type": "table",
                "page": 18,
                "parent_chapter": 261,
                "index": 268,
                "outline": [
                    79.0,
                    215.0,
                    513.5,
                    595.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "title": "Table 24: Examples of financial QA pairs categorized by question type. The highlighted segments in the evidence indicate the spans that are most relevant to the question.",
                "title_index": 269,
                "page_merged_table": null,
                "cells": {
                    "0_0": {
                        "text": "Category"
                    },
                    "0_1": {
                        "text": " Examples"
                    },
                    "1_0": {
                        "text": "Numeric (Table)"
                    },
                    "1_1": {
                        "text": "Question: What percentage of the estimated purchase price for Hologic in 2007 is\nattributed to the net tangible assets?\nAnswer: 3.8%\nEvidence: HOLX_2007_10K (p.110)\nThe components and initial allocation of the purchase price consist of:\nNet tangible assets acquired as of September 18, 2007: $2,800\nDeveloped technology and know how: $12,300\nCustomer relationship: $17,000\nTrade name: $2,800\nEstimated Purchase Price: $73,200."
                    },
                    "2_0": {
                        "text": "Numeric (Text)"
                    },
                    "2_1": {
                        "text": "Question: During the years 2004 to 2006, what was the average impairment on con-\nstruction in progress, expressed in millions, for American Tower Corporation as reported\nin their 2006 financial documents?\nAnswer: 2.63\nEvidence: AMT_2006_10K (p.106)\nConstruction-In-Progress Impairment Charges—For the years ended De-\ncember 31, 2006, 2005 and 2004, the Company wrote-off approximately\n$1.0 million, $2.3 million and $4.6 million , respectively, of construction-in-progress\ncosts, primarily associated with sites that it no longer planned to build."
                    },
                    "3_0": {
                        "text": "Textual"
                    },
                    "3_1": {
                        "text": "Question: What are the geographies that American Express primarily operates in as of\n2022?\nAnswer: United States, EMEA, APAC, and LACC\nEvidence: AXP_2022_10K (p.154)\nEffective for the first quarter of 2022, we changed the way in which we allocate certain\noverhead expenses by geographic region. As a result, prior period pretax income (loss)\nfrom continuing operations by geography has been recast to conform to current period\npresentation there was no impact at a consolidated level.\n(Millions) United States EMEA APAC LACC Other Unallocated Consolidated\n2022\nTotal revenues net of interest expense 41, 396 4,871 3, 835 2,917 (157) 52,862"
                    }
                },
                "merged": [],
                "grid": {
                    "rows": [
                        17.5,
                        146.0,
                        251.8
                    ],
                    "columns": [
                        91.7
                    ]
                }
            },
            {
                "type": "paragraph",
                "text": "Table 24: Examples of financial QA pairs categorized by question type. The highlighted segments in the evidence indicate the spans that are most relevant to the question.",
                "page": 18,
                "parent_chapter": -1,
                "index": 269,
                "outline": [
                    70.0,
                    602.0,
                    524.5,
                    625.5
                ],
                "is_chapter_title": false,
                "rotation": 0.0,
                "continued": false,
                "page_merged_paragraph": null
            }
        ]
    }
}